{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize Google News Results with LangChainðŸ¦œðŸ”—, HuggingfaceðŸ¤— and Serper API\n",
    "\n",
    "## Overview\n",
    "\n",
    "Text summarization is the process of creating a shorter version of a text document while still preserving the most important information. This can be useful for a variety of purposes, such as quickly skimming a long document, getting the gist of an article, or sharing a summary with others. LLMs can be used to create summaries of news articles, research papers, technical documents, and other types of text.\n",
    "\n",
    "<img src=\"images/miztiik_text_summarization_01.png\" width=\"50%\"/>\n",
    "\n",
    "In this notebook, we will try fetch the latest Google news using server API and use AI-generated summaries with LangChain LLM framework or huggingface transformers.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/miztiik/llm-bootcamp/blob/main/chapters/text_summarization/news_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Comment the above line to see the installation logs\n",
    "\n",
    "# Install the dependencies\n",
    "!pip install -qU python-dotenv\n",
    "!pip install -qU langchain\n",
    "!pip install -qU langchain-openai\n",
    "!pip install -qU newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not a good practice, but we will ignore warnings in this notebook, as tensor has deprecated some methods and will be removed in future versions.\n",
    "# https://github.com/pytorch/pytorch/issues/97207#issuecomment-1494781560\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=UserWarning, message=\"TypedStorage is deprecated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update your `API_KEY` in the `.env` file. You can get the API keys from the following links. _Note: Some of the services may require you to have an account and some may charge you for usage_\n",
    "- [OpenAI API Key](https://platform.openai.com/account/api-keys)\n",
    "- [Hugging Face API Key](https://huggingface.co/settings/tokens)\n",
    "- [Serper API Key](https://serper.dev/api-key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# To specify a particular model refer to the OpenAI documentation - https://platform.openai.com/docs/models\n",
    "# Completions Model: https://platform.openai.com/docs/models/completions\n",
    "# Chat Model: https://platform.openai.com/docs/models/completions\n",
    "\n",
    "llm = OpenAI()\n",
    "llm_chat = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.document_loaders import NewsURLLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Serper API** - [Sign up](https://serper.dev/signup?ref=miztiik) for an account with Serper, or log in if you already have an account, and create an API key. Serper offers a generous free tier; as you consume the API, the dashboard will populate with the requests and remaining credits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "search = GoogleSerperAPIWrapper(\n",
    "    type=\"news\", tbs=\"qdr:d1\", serper_api_key=os.getenv(\"SERPER_API_KEY\")\n",
    ")\n",
    "\n",
    "news_search_query = \"indian economy trends\"\n",
    "news_results = search.results(news_search_query)\n",
    "\n",
    "if news_results.get(\"news\") is None:\n",
    "    print(\"No results found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(news_results[\"news\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in news_results[\"news\"][0]:\n",
    "    print(f\"{i}:{news_results['news'][0][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit how many news articles to process\n",
    "num_results = 5\n",
    "if len(news_results[\"news\"]) < num_results:\n",
    "    num_results = len(news_results[\"news\"])\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \"], chunk_size=10000, chunk_overlap=100)\n",
    "\n",
    "\n",
    "# For each news article, load the contents\n",
    "for i in range(num_results):\n",
    "    # loader = WebBaseLoader(news_results[\"news\"][i][\"link\"])\n",
    "    loader = NewsURLLoader(urls=[news_results[\"news\"][i][\"link\"]])\n",
    "    news_results[\"news\"][i][\"contents\"] = loader.load()\n",
    "\n",
    "    # Make the docs to fit model input size\n",
    "    news_results[\"news\"][i][\"contents\"] = text_splitter.create_documents(\n",
    "        [news_results[\"news\"][i][\"contents\"][0].page_content])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization with Open AI Models\n",
    "\n",
    "<img src=\"images/miztiik_text_summarization_02.png\" width=\"50%\"/>\n",
    "\n",
    "If our document is short enough to fit within the context window of the model, we can use the `stuffing` method.\n",
    "\n",
    "**Stuffing method** - The `stuffing` method is the easiest way to summarize text by feeding the entire document to a large language model (LLM) in a single call. This method has both pros and cons.\n",
    "\n",
    "  - **Pros**:\n",
    "    - Only required a single call to the model, which can be faster than other methods that require multiple calls\n",
    "    - When summarizing text, the model has access to all the data at once, which can result in a better summary.\n",
    "  - **Cons**:\n",
    "    - Most models have a context length, and for large documents (or many documents) this will not work as it will result in a prompt larger than the context length.\n",
    "    - This method only works on smaller pieces of data and not suitable to large documents most of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16k is the max input length for GPT-3.5\n",
    "num_tokens_first_doc = llm.get_num_tokens(\n",
    "    news_results[\"news\"][1][\"contents\"][0].page_content\n",
    ")\n",
    "\n",
    "oai_chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='map_reduce',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "oai_summary = oai_chain.invoke(news_results[\"news\"][1][\"contents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Title: {news_results['news'][1]['title']}\\n\\nLink: {news_results['news'][1]['link']}\\n\\nSummary: \\033[32m{oai_summary['output_text']}\\033[0m\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "The summarization is not perfect, but it does a good job of capturing the main points of the article. The summary is coherent and reads well. As OpenAI continues to improve their models, we can expect the quality of the summaries to improve as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization with Open Source Models\n",
    "\n",
    "<img src=\"images/miztiik_text_summarization_03.png\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization with Hugging Face hosted models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.llms.huggingface_hub import HuggingFaceHub\n",
    "\n",
    "hf_hub_llm = HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\": 0.3, \"max_length\": 256}\n",
    ")\n",
    "\n",
    "chain = load_summarize_chain(\n",
    "    llm=hf_hub_llm,\n",
    "    chain_type='map_reduce',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# run chain\n",
    "map_reduce_summary = chain.invoke(news_results[\"news\"][1][\"contents\"])\n",
    "\n",
    "print(map_reduce_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, max_chunk_size=2048):\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sentence in text.split(\".\"):\n",
    "        if len(current_chunk) + len(sentence) < max_chunk_size:\n",
    "            current_chunk += sentence + \".\"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \".\"\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_news_txt = split_text(\n",
    "    news_results[\"news\"][1][\"contents\"][0].page_content)\n",
    "\n",
    "# print(len(chunked_news_txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization with Open Source Local Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the model and tokenizer\n",
    "\n",
    "bart_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "bart_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "print(f\"bart_model_max_model_length:{bart_tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_summarizer = pipeline(\n",
    "    \"summarization\", model=\"facebook/bart-large-cnn\", truncation=True\n",
    ")\n",
    "\n",
    "bart_summary = bart_summarizer(\n",
    "    news_results[\"news\"][1][\"contents\"][0].page_content,\n",
    "    max_length=300,\n",
    "    min_length=10\n",
    ")\n",
    "\n",
    "print(bart_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization with Open Source Local Smaller Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_small_model_summarizer = pipeline(\n",
    "    \"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", truncation=True)\n",
    "\n",
    "\n",
    "\n",
    "hf_small_model_summary = hf_small_model_summarizer(\n",
    "    news_results[\"news\"][1][\"contents\"][0].page_content, max_length=250, min_length=10\n",
    ")\n",
    "\n",
    "print(hf_small_model_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Reading\n",
    "\n",
    "- [LLM Bootcamp](https://github.com/miztiik/llm-bootcamp)\n",
    "- [Revolutionizing News Summarization](https://www.width.ai/post/revolutionizing-news-summarization-exploring-the-power-of-gpt-in-zero-shot-and-specialized-tasks)\n",
    "- [Summarizer For Any Size Document](https://www.width.ai/post/gpt3-summarizer)\n",
    "- [Langchain Summarization - Stuff & Map Reduce](https://python.langchain.com/docs/use_cases/summarization)\n",
    "- [Langchain Google Serper](https://python.langchain.com/docs/integrations/tools/google_serper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
