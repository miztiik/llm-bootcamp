{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# What is Langchain\n",
        "Langchain is an open-source framework enabling developers to integrate large language models like GPT-4 with external computation and data sources. It is available as a Python or JavaScript/TypeScript package.\n",
        "\n",
        "## Why Langchain is needed:\n",
        "Langchain is essential for connecting large language models to personalized data sources, such as books, PDFs, or databases, allowing users to interact with their own information dynamically. It facilitates the creation of data-aware and authentic applications, opening up various practical use cases, including personal assistance, learning, coding, data analysis, and connecting language models to company data for advanced analytics. The framework's main value proposition lies in LLW wrappers, prompt templates, chains, and agents, enabling seamless integration and interaction with language models.\n",
        "\n",
        "**References**\n",
        "- [LangChain](https://python.langchain.com/docs/get_started/quickstart)\n",
        "\n",
        "In this notebook, we will use the `langchain` library to use pre-trained models for various NLP tasks. We will use the `pipeline` class to use pre-trained models for various NLP tasks. The `pipeline` class provides a simple API dedicated to several NLP tasks. It provides a simple, straight-forward, and efficient way to use pre-trained models.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/miztiik/llm-bootcamp/blob/main/chapters/intro_to_langchain/intro_to_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Comment the above line to see the installation logs\n",
        "\n",
        "# Install the dependencies\n",
        "!pip install -qU python-dotenv\n",
        "!pip install -qU langchain\n",
        "!pip install -qU langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "50dvxjqCFmhF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load environment variables\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "Models"
        ]
      },
      "source": [
        "## LangChain: Model I/O - Text Model Wrapper\n",
        "\n",
        "Update your `OPENAI_API_KEY` in the `.env` file to use the OpenAI model. You can get the API key from the [OpenAI website](https://platform.openai.com/account/api-keys)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "C7RnyUOCJWmk"
      },
      "outputs": [],
      "source": [
        "# Run basic query with OpenAI wrapper\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "llm = OpenAI()\n",
        "\n",
        "# To specify a particular model refer to the OpenAI documentation - https://platform.openai.com/docs/models\n",
        "# Completions Model: https://platform.openai.com/docs/models/completions\n",
        "# Chat Model: https://platform.openai.com/docs/models/completions\n",
        "\n",
        "\n",
        "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\nThe currency of India is the Indian rupee (INR). '"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.invoke(\"What is the currency of india\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Large language models are powerful artificial intelligence systems that can process and generate human-like text by training on vast amounts of data.\n"
          ]
        }
      ],
      "source": [
        "txt_resp = llm.invoke(\"explain large language models in one sentence\")\n",
        "print(txt_resp, end=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can pass multiple text prompts to an OpenAI model via the `generate()` method. For example, the following script returns two outputs, one for each prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "multiple_txt_resp = llm.generate(\n",
        "    [\n",
        "        \"What is the capital of india\",\n",
        "        \"Tell me a joke about AI\",\n",
        "        \"Who won FIFA 2018\",  # Change it to 2022, and see what happens\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total responses: 3\n",
            "Response 1: \n",
            "\n",
            "The capital of India is New Delhi.\n",
            "Response 2: \n",
            "\n",
            "Why did the AI cross the road?\n",
            "\n",
            "To get to the other algorithm.\n",
            "Response 3: ?\n",
            "\n",
            "France won the FIFA World Cup 2018.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total responses: {len(multiple_txt_resp.generations)}\")\n",
        "\n",
        "for i, resp in enumerate(multiple_txt_resp.generations):\n",
        "    print(f\"Response {i+1}: {resp[0].text}\", end=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\nWhy did the robot go on a diet?\\n\\nBecause he wanted to reduce his \"byte\" size!'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multiple_txt_resp.generations[1][0].text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LangChain: Model I/O - Chat Model Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"The player of the tournament in the 1998 FIFA World Cup was Ronaldo of Brazil. Despite Brazil losing in the final to France, Ronaldo's performances throughout the tournament earned him the prestigious award.\")"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Chat Model: https://platform.openai.com/docs/models/completions\n",
        "\n",
        "llm_chat = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=0.3)\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are an football historian\"),\n",
        "    HumanMessage(\n",
        "        content=\"Who won the player of the tournament in 1998 Fifa World Cup?\"),\n",
        "]\n",
        "\n",
        "\n",
        "chat_resp = llm_chat.invoke(messages)\n",
        "chat_resp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Stream the output from the chat model to the console."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ronaldo of Brazil won the Golden Ball award as the best player of the tournament in the 1998 FIFA World Cup. Despite Brazil losing to France in the final, Ronaldo's performances throughout the tournament earned him the prestigious award."
          ]
        }
      ],
      "source": [
        "for chunk in llm_chat.stream(messages):\n",
        "    print(chunk.content, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Langchain: Prompt Templates\n",
        "\n",
        "A prompt for a language model is a set of instructions or input provided by a user to guide the model's response, helping it understand the context and generate relevant and coherent language-based output, such as answering questions, completing sentences, or engaging in a conversation.\n",
        "\n",
        "- String prompt template\n",
        "- Chat prompt templates\n",
        "\n",
        "Source: https://python.langchain.com/docs/modules/model_io/prompts/quick_start\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### String Prompt Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Tell me a funny joke about AI.'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Tell me a {adjective} joke about {content}.\"\n",
        ")\n",
        "prompt_template.format(adjective=\"funny\", content=\"AI\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Why did the tech engineer quit his job at the computer factory? \n",
            "\n",
            "Because he couldn't handle all the hardware failures.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "joke_resp = llm.invoke(prompt_template.format(\n",
        "    adjective=\"sad\", content=\"Tech Engineer\"))\n",
        "\n",
        "print(joke_resp, end=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['concept'], template='\\nYou are an expert data scientist with an expertise in building deep learning models. \\nExplain the concept of {concept} in a couple of lines.\\n', validate_template=True)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "template = \"\"\"\n",
        "You are an expert data scientist with an expertise in building deep learning models. \n",
        "Explain the concept of {concept} in a couple of lines.\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"concept\"],\n",
        "    template=template,\n",
        "    template_format='f-string',\n",
        "    validate_template=True\n",
        ")\n",
        "\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generative models are a type of machine learning model that learns the underlying patterns and structures of a dataset. These models can then be used to generate new data that is similar to the original dataset. They are commonly used in tasks such as image and text generation, and can also be used for tasks like anomaly detection and data augmentation.\n"
          ]
        }
      ],
      "source": [
        "# Run LLM with PromptTemplate\n",
        "prompt_template_resp = llm.invoke(prompt.format(concept=\"generative models\"))\n",
        "print(prompt_template_resp, end=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Large Language Models (LLMs) refer to deep learning models that are trained on a large amount of text data and can generate human-like text responses. These models use advanced techniques such as attention mechanisms and transformer architectures to understand the context of the given text and produce coherent and relevant responses. LLMs are often used in natural language processing tasks such as text completion, question-answering, and text summarization.\n"
          ]
        }
      ],
      "source": [
        "# Try another query with PromptTemplate\n",
        "prompt_template_resp = llm.invoke(\n",
        "    prompt.format(concept=\"Large Language Models\"))\n",
        "print(prompt_template_resp, end=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chat prompt composition\n",
        "\n",
        "Source: \n",
        "- https://python.langchain.com/docs/modules/model_io/chat/quick_start#messages\n",
        "- https://python.langchain.com/docs/modules/model_io/prompts/composition#chat-prompt-composition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtHgQ5XpJmgi"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import HumanMessage, SystemMessage\n",
        "from langchain.prompts import HumanMessagePromptTemplate\n",
        "\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[SystemMessage(content='You are a Tennis historian.'), HumanMessage(content='Who won the Australian Open in 2015')]\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        # SystemMessage(content=(\"You are a {sports} historian.\")),\n",
        "        SystemMessagePromptTemplate.from_template(\n",
        "            \"You are a {sports} historian.\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chat_prompt = chat_template.format_prompt(\n",
        "    sports=\"Tennis\",\n",
        "    text=\"Who won the Australian Open in 2015\").to_messages()\n",
        "\n",
        "print(chat_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The winners of the Australian Open in 2015 were Novak Djokovic in the men's singles category and Serena Williams in the women's singles category.\n"
          ]
        }
      ],
      "source": [
        "prompt_resp = llm_chat.invoke(chat_prompt)\n",
        "print(prompt_resp.content, end=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content=\"The 2018 FIFA World Cup was won by the French national team. They defeated Croatia 4-2 in the final match, which took place on July 15, 2018, at the Luzhniki Stadium in Moscow, Russia. This victory marked France's second World Cup title, with their first win coming in 1998 when they hosted the tournament.\"\n"
          ]
        }
      ],
      "source": [
        "# Try another query with ChatPromptTemplate\n",
        "print(llm_chat.invoke(\n",
        "    chat_template.format_prompt(\n",
        "        sports=\"Football\", text=\"Who won the 2018 FIFA World Cup?\").to_messages()\n",
        ")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Langchain: Chains\n",
        "\n",
        "Chains allow you to run multiple LangChain modules in conjunction. For example, using a chain, you can run a prompt and an LLM together, saving you from first formatting a prompt for an LLM model and executing it using the model in separate steps.\n",
        "\n",
        "LangChain supports three main types of chains:\n",
        "\n",
        "- Simple LLM Chain\n",
        "- Sequential Chain\n",
        "- Custom Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an expert technical advisor.\"),\n",
        "        (\"user\", \"{input}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Sure, here's a Python script that generates a list of prime numbers up to 100:\\n\\n```python\\ndef is_prime(n):\\n    if n <= 1:\\n        return False\\n    for i in range(2, int(n**0.5) + 1):\\n        if n % i == 0:\\n            return False\\n    return True\\n\\nprimes = []\\nfor num in range(2, 101):\\n    if is_prime(num):\\n        primes.append(num)\\n\\nprint(primes)\\n```\\n\\nThis script defines a function `is_prime()` which checks if a given number is prime. It iterates from 2 to the square root of the number (inclusive) and if any divisor is found, it returns `False`. Otherwise, it returns `True`.\\n\\nThen, it loops through numbers from 2 to 100 and checks if each number is prime using the `is_prime()` function. If a number is prime, it is appended to the `primes` list.\\n\\nFinally, the script prints the list of prime numbers.\")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke(\n",
        "    {\"input\": \"Write a Python script to generate a list of prime numbers up to 100.?\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2grf7I8AJ_hK"
      },
      "outputs": [],
      "source": [
        "# Import prompt and define PromptTemplate\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "You are an expert data scientist with an expertise in building deep learning models. \n",
        "Explain the concept of {concept} in a couple of lines\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"concept\"],\n",
        "    template=template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcz7Q9Y-KFvI"
      },
      "outputs": [],
      "source": [
        "# Run LLM with PromptTemplate\n",
        "\n",
        "llm(prompt.format(concept=\"autoencoder\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dm78i-rUKXIB"
      },
      "outputs": [],
      "source": [
        "# Import LLMChain and define chain with language model and prompt as arguments.\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Run the chain only specifying the input variable.\n",
        "print(chain.run(\"autoencoder\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6MF4-nMKul3"
      },
      "outputs": [],
      "source": [
        "# Define a second prompt\n",
        "\n",
        "second_prompt = PromptTemplate(\n",
        "    input_variables=[\"ml_concept\"],\n",
        "    template=\"Turn the concept description of {ml_concept} and explain it to me like I'm five in 500 words\",\n",
        ")\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkJKFyk1K-MO"
      },
      "outputs": [],
      "source": [
        "# Define a sequential chain using the two chains above: the second chain takes the output of the first chain as input\n",
        "\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
        "\n",
        "# Run the chain specifying only the input variable for the first chain.\n",
        "explanation = overall_chain.run(\"autoencoder\")\n",
        "print(explanation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDDu1B_SLQls"
      },
      "outputs": [],
      "source": [
        "# Import utility for splitting up texts and split up the explanation given above into document chunks\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=0,\n",
        ")\n",
        "\n",
        "texts = text_splitter.create_documents([explanation])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6lfAdeuLhtp"
      },
      "outputs": [],
      "source": [
        "# Individual text chunks can be accessed with \"page_content\"\n",
        "\n",
        "texts[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5sv4e3tLw2y"
      },
      "outputs": [],
      "source": [
        "# Import and instantiate OpenAI embeddings\n",
        "\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model_name=\"ada\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dqzoir4hMlfl"
      },
      "outputs": [],
      "source": [
        "# Turn the first text chunk into a vector with the embedding\n",
        "\n",
        "query_result = embeddings.embed_query(texts[0].page_content)\n",
        "print(query_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaOY5bIZM3Xz"
      },
      "outputs": [],
      "source": [
        "# Import and initialize Pinecone client\n",
        "\n",
        "import os\n",
        "import pinecone\n",
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "\n",
        "pinecone.init(\n",
        "    api_key=os.getenv(\"PINECONE_API_KEY\"), environment=os.getenv(\"PINECONE_ENV\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZhSUt3FNBzN"
      },
      "outputs": [],
      "source": [
        "# Upload vectors to Pinecone\n",
        "\n",
        "index_name = \"langchain-quickstart\"\n",
        "search = Pinecone.from_documents(texts, embeddings, index_name=index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCXVuXwPNKcc"
      },
      "outputs": [],
      "source": [
        "# Do a simple vector similarity search\n",
        "\n",
        "query = \"What is magical about an autoencoder?\"\n",
        "result = search.similarity_search(query)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ogz8luZNRnJ"
      },
      "outputs": [],
      "source": [
        "# Import Python REPL tool and instantiate Python agent\n",
        "\n",
        "from langchain.agents.agent_toolkits import create_python_agent\n",
        "from langchain.tools.python.tool import PythonREPLTool\n",
        "from langchain.python import PythonREPL\n",
        "from langchain.llms.openai import OpenAI\n",
        "\n",
        "agent_executor = create_python_agent(\n",
        "    llm=OpenAI(temperature=0, max_tokens=1000), tool=PythonREPLTool(), verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVHMDj0sNi09"
      },
      "outputs": [],
      "source": [
        "# Execute the Python agent\n",
        "\n",
        "agent_executor.run(\n",
        "    \"Find the roots (zeros) if the quadratic function 3 * x**2 + 2*x -1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Langchain: Custom Models\n",
        "\n",
        "C transformers package implements various LLMs that you can use in LangChain. You do not need an API key to access C transformers LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install -qU CTransformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.llms import CTransformers\n",
        "\n",
        "llm = CTransformers(model='marella/gpt-2-ggml')\n",
        "print(llm(\"I am flying to Lisbon on\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyONM96f7/m0jUCD9c87+MQy",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
