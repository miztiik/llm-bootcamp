{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# Langchain Models\n",
        "LangChain supports three types of models:\n",
        "\n",
        "- Language Models\n",
        "  - LLM\n",
        "  - Chat Models\n",
        "- Text Embedding Models\n",
        "\n",
        "  ![LangChain Use Cases](./images/models.png)\n",
        "\n",
        "In this notebook, we will use the `langchain` library to use pre-trained models for various NLP tasks.\n",
        "    \n",
        "**References**\n",
        "- [LangChain Models](https://python.langchain.com/docs/modules/model_io/quick_start)\n",
        "\n",
        "\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/miztiik/llm-bootcamp/blob/main/chapters/intro_to_langchain/langchain_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Comment the above line to see the installation logs\n",
        "\n",
        "# Install the dependencies\n",
        "!pip install -qU python-dotenv\n",
        "!pip install -qU langchain\n",
        "!pip install -qU langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50dvxjqCFmhF"
      },
      "outputs": [],
      "source": [
        "# Load environment variables\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "Models"
        ]
      },
      "source": [
        "## LangChain: Model I/O - LLM Model\n",
        "\n",
        "Update your `OPENAI_API_KEY` in the `.env` file to use the OpenAI model. You can get the API key from the [OpenAI website](https://platform.openai.com/account/api-keys)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7RnyUOCJWmk"
      },
      "outputs": [],
      "source": [
        "# Run basic query with OpenAI wrapper\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "llm = OpenAI()\n",
        "\n",
        "# To specify a particular model refer to the OpenAI documentation - https://platform.openai.com/docs/models\n",
        "# Completions Model: https://platform.openai.com/docs/models/completions\n",
        "# Chat Model: https://platform.openai.com/docs/models/completions\n",
        "\n",
        "\n",
        "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm.invoke(\"What is the currency of india\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt_resp = llm.invoke(\"explain large language models in one sentence\")\n",
        "print(txt_resp, end=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can pass multiple text prompts to an OpenAI model via the `generate()` method. For example, the following script returns two outputs, one for each prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "multiple_txt_resp = llm.generate(\n",
        "    [\n",
        "        \"What is the capital of india\",\n",
        "        \"Tell me a joke about AI\",\n",
        "        \"Who won FIFA 2018\",  # Change it to 2022, and see what happens\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Total responses: {len(multiple_txt_resp.generations)}\")\n",
        "\n",
        "for i, resp in enumerate(multiple_txt_resp.generations):\n",
        "    print(f\"Response {i+1}: {resp[0].text}\", end=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "multiple_txt_resp.generations[1][0].text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LangChain: Model I/O - Chat Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chat Model: https://platform.openai.com/docs/models/completions\n",
        "\n",
        "llm_chat = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=0.3)\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are an football historian\"),\n",
        "    HumanMessage(\n",
        "        content=\"Who won the player of the tournament in 1998 Fifa World Cup?\"),\n",
        "]\n",
        "\n",
        "\n",
        "chat_resp = llm_chat.invoke(messages)\n",
        "chat_resp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Stream the output from the chat model to the console."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for chunk in llm_chat.stream(messages):\n",
        "    print(chunk.content, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Langchain Text Embedding\n",
        "\n",
        "The LangChain text embedding models return numeric representations of text inputs that you can use to train statistical algorithms such as machine learning models.\n",
        "\n",
        "  ![LangChain Use Cases](./images/text_embedding.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "text = \"I enjoy long walks.\"\n",
        "embeddings_result = embeddings.embed_query(text)\n",
        "\n",
        "print(embeddings_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Embedding multiple text inputs at once is more efficient than embedding them one by one. The `embed_documents()` method accepts a list of text inputs and returns a list of embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_txt = [\n",
        "    \"GDP of India is around 3 Trillion\",\n",
        "    \"Apple market cap is around 3 Trillion\",\n",
        "    \"Top 10 IT companies revenue is around 3 Trillion\",\n",
        "]\n",
        "\n",
        "batch_embeddings_result = embeddings.embed_documents(batch_txt)\n",
        "\n",
        "print(f\"Total Embeddings:{len(batch_embeddings_result)}\")\n",
        "print(f\"First Sentence Embedding: {batch_embeddings_result[0]}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyONM96f7/m0jUCD9c87+MQy",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
