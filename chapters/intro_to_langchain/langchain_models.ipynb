{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# Langchain Models\n",
        "LangChain supports three types of models:\n",
        "\n",
        "- Language Models\n",
        "  - LLM\n",
        "  - Chat Models\n",
        "- Text Embedding Models\n",
        "\n",
        "  <img src=\"images/model_io.jpg\" alt=\"Langchain Model IO\" width=50%/>\n",
        "\n",
        "In this notebook, we will use the `langchain` library to use pre-trained models for various NLP tasks.\n",
        "    \n",
        "<a href=\"https://colab.research.google.com/github/miztiik/llm-bootcamp/blob/main/chapters/intro_to_langchain/langchain_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Comment the above line to see the installation logs\n",
        "\n",
        "# Install the dependencies\n",
        "!pip install -qU python-dotenv\n",
        "!pip install -qU langchain\n",
        "!pip install -qU langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "50dvxjqCFmhF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load environment variables\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "Models"
        ]
      },
      "source": [
        "## LangChain: Model I/O - LLM Model\n",
        "\n",
        "Update your `OPENAI_API_KEY` in the `.env` file to use the OpenAI model. You can get the API key from the [OpenAI website](https://platform.openai.com/account/api-keys)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7RnyUOCJWmk"
      },
      "outputs": [],
      "source": [
        "# Run basic query with OpenAI wrapper\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "llm = OpenAI()\n",
        "\n",
        "# To specify a particular model refer to the OpenAI documentation - https://platform.openai.com/docs/models\n",
        "# Completions Model: https://platform.openai.com/docs/models/completions\n",
        "# Chat Model: https://platform.openai.com/docs/models/completions\n",
        "\n",
        "\n",
        "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm.invoke(\"What is the currency of india\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt_resp = llm.invoke(\"explain large language models in one sentence\")\n",
        "print(txt_resp, end=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can pass multiple text prompts to an OpenAI model via the `generate()` method. For example, the following script returns two outputs, one for each prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "multiple_txt_resp = llm.generate(\n",
        "    [\n",
        "        \"What is the capital of india\",\n",
        "        \"Tell me a joke about AI\",\n",
        "        \"Who won FIFA 2018\",  # Change it to 2022, and see what happens\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Total responses: {len(multiple_txt_resp.generations)}\")\n",
        "\n",
        "for i, resp in enumerate(multiple_txt_resp.generations):\n",
        "    print(f\"Response {i+1}: {resp[0].text}\", end=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "multiple_txt_resp.generations[1][0].text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LangChain: Model I/O - Chat Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chat Model: https://platform.openai.com/docs/models/completions\n",
        "\n",
        "llm_chat = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=0.3)\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are an football historian\"),\n",
        "    HumanMessage(\n",
        "        content=\"Who won the player of the tournament in 1998 Fifa World Cup?\"),\n",
        "]\n",
        "\n",
        "\n",
        "chat_resp = llm_chat.invoke(messages)\n",
        "chat_resp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Stream the output from the chat model to the console."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for chunk in llm_chat.stream(messages):\n",
        "    print(chunk.content, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Langchain Text Embedding\n",
        "\n",
        "The LangChain text embedding models return numeric representations of text inputs that you can use to train statistical algorithms such as machine learning models.\n",
        "\n",
        "  <img src=\"images/text_embedding.png\" alt=\"Langchain Text Embedding\" width=50%/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Embedding with OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "text = \"I enjoy long walks.\"\n",
        "embeddings_result = embeddings.embed_query(text)\n",
        "\n",
        "print(embeddings_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Embedding multiple text inputs at once is more efficient than embedding them one by one. The `embed_documents()` method accepts a list of text inputs and returns a list of embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_txt = [\n",
        "    \"GDP of India is around 3 Trillion\",\n",
        "    \"Apple market cap is around 3 Trillion\",\n",
        "    \"Top 10 IT companies revenue is around 3 Trillion\",\n",
        "]\n",
        "\n",
        "batch_embeddings_result = embeddings.embed_documents(batch_txt)\n",
        "\n",
        "print(f\"Total Embeddings:{len(batch_embeddings_result)}\")\n",
        "print(f\"First Sentence Embedding: {batch_embeddings_result[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Embedding with OpenSource Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "\n",
        "embeddings = SentenceTransformerEmbeddings(\n",
        "    model_name=\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_txt = [\n",
        "    \"Apple's annual revenue was approximately 365 billion dollars in 2021.\",\n",
        "    \"Microsoft's annual revenue was approximately 168 billion dollars in 2021.\",\n",
        "    \"Amazon's annual revenue was approximately 470 billion dollars in 2021.\",\n",
        "    \"In 2020, Europe traded â‚¬2,270 million worth of apples.\",\n",
        "    \"In 2030, European Per capita consumption of apples expected to increase to 15 kg .\",\n",
        "    \"An average Papua New Guinean eats 100Kg of banana's annually.\"\n",
        "]\n",
        "\n",
        "\n",
        "batch_embeddings_result = embeddings.embed_documents(batch_txt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Langchain: Custom Models\n",
        "\n",
        "C transformers package implements various LLMs that you can use in LangChain. You do not need an API key to access C transformers LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install -qU CTransformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.llms import CTransformers\n",
        "\n",
        "llm = CTransformers(model='marella/gpt-2-ggml')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " a big weekend and will be meeting up with some friends of mine on Thursday evening. We were there about 1 pm today at my aunt's house so it would probably make the best first contact if anyone knew how far along we are! The rest of you know, I'll check out here (for all kinds of great food & music):\n",
            "\n",
            "\n",
            "Mooi O-Yuk â€“ Lufkin on Goulart with Vint Mijen â€” Nourky de Mezzurie DÃ¼rema a Sowmeister kultrunk van mÃ¸der german zoor B. Picha nagwara boudenspiedraet du (Ochlundt-Feder.) Zegern â€“ \"Catch Up On Some Fresh Foods With Us\" with John Doe and other DÃ¼sseldorf based restaurants & moreâ€¦\n",
            "\n",
            "\n",
            "We've already mentioned this first two months back that there was no news regarding us on social media as I just want to tell you something about the people who are here. We have been in touch for a while now, but we haven't had any contact with them yet and they're all very busy so it's not like everyone is getting together at once!\n",
            "\n",
            "\n",
            "We\n"
          ]
        }
      ],
      "source": [
        "print(llm.invoke(\"I am flying to Amsterdam for\", num_return_sequences=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Reading\n",
        "\n",
        "- [LangChain Models](https://python.langchain.com/docs/modules/model_io/quick_start)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyONM96f7/m0jUCD9c87+MQy",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
