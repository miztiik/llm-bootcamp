{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# Langchain Retrieval\n",
        "\n",
        "- Document Loaders\n",
        "- Text Splitting\n",
        "- Vector stores\n",
        "- Retreivers\n",
        "- few more tools..\n",
        "  \n",
        "<img src=\"images/langchain_retrieval.jpg\" width=75%/>\n",
        "\n",
        "In this notebook, we will use the `langchain` library to use pre-trained models for various NLP tasks. \n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/miztiik/llm-bootcamp/blob/main/chapters/intro_to_langchain/langchain_retreival.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Comment the above line to see the installation logs\n",
        "\n",
        "# Install the dependencies\n",
        "!pip install -qU python-dotenv\n",
        "!pip install -qU langchain\n",
        "!pip install -qU langchain-openai\n",
        "!pip install -qU pypdf\n",
        "!pip install -qU unstructured\n",
        "!pip install -qU unstructured[md]\"\n",
        "!pip install -qU rapidocr-onnxruntime\n",
        "!pip install -qU chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -qU unstructured[md]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50dvxjqCFmhF"
      },
      "outputs": [],
      "source": [
        "# Load environment variables\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7RnyUOCJWmk"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")\n",
        "llm_chat = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Document loaders\n",
        "\n",
        "Download any reasonably sized pdf and upload it to the colab environment. We will use the `langchain` library to load the document and extract the text from it. You can find some samples in the `datasets` folder in the repo.\n",
        "\n",
        "<img src=\"images/langchain_retrieval.jpg\" width=40%/>\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\n",
        "    \"./../../datasets/raw_data/pdf/2023_india_economic_survey.pdf\")\n",
        "pages = loader.load_and_split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets check out the few pages from the document. An advantage of this approach is that documents can be retrieved with page numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pages[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extract text from images in PDF\n",
        "\n",
        "Using the `rapidocr-onnxruntime` package we can extract images as text as well:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -qU rapidocr-onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\n",
        "    \"https://arxiv.org/pdf/2103.15348.pdf\", extract_images=True)\n",
        "pages = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pages[4].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader = PyPDFLoader(\n",
        "    \"./../../datasets/raw_data/pdf/bain_on_strategy.pdf\", extract_images=True\n",
        ")\n",
        "pages = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pages[3].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading all documents in a folder based on extension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "\n",
        "\n",
        "loader = DirectoryLoader(\"./../../datasets/raw_data/\", glob=\"**/*.md\")\n",
        "\n",
        "\n",
        "docs = loader.load()\n",
        "\n",
        "\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
        "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
        "\n",
        "# For Markdown\n",
        "loader = UnstructuredMarkdownLoader(\"README.md\")\n",
        "# For HTML\n",
        "loader = UnstructuredHTMLLoader(\"index.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Splitters\n",
        "\n",
        "- **Character Splitting** - Simple static character chunks of data\n",
        "  - The problem with it is that we do not take into account the structure of our document at all. We simply split by a fixed number of characters.\n",
        "- **Recursive Character Text Splitting** - Recursive chunking based on a list of separators. We can specify a list of separators. This is the swiss army knife of splitters and my first choice, when you don't know which splitter to start with, this is a good first bet.\n",
        "- **Document Specific Splitting** - Various chunking methods for different document types (PDF, Python, Markdown)\n",
        "\n",
        "\n",
        "Additional Reading - [5 Levels Of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/5_Levels_Of_Text_Splitting.ipynb)\n",
        "\n",
        "\n",
        "Lets try these on some sample text,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_txt = \"\"\"\n",
        "The economic prosperity of the Tamils depended on foreign trade. Literary, archaeological and numismatic sources confirm the trade relationship between Tamilakam and Rome, where spices and pearls from India were in great demand. With the accession of Augustus in 27 BCE, trade between Tamilakam and Rome received a tremendous boost and culminated at the time of Nero who died in 68 CE. At that point, trade declined until the death of Caracalla (217 CE), after which it almost ceased. It was revived again under the Byzantine emperors. Under the early Roman emperors, there was a great demand for articles of luxury, especially beryl. \n",
        "\n",
        "Most of the articles of luxury mentioned by the Roman writers came from Tamilakam. In the declining period, cotton and industrial products were still imported by Rome. The exports from the Tamil country included pepper, pearls, ivory, textiles and gold ornaments, while the imports were luxury goods such as glass, coral, wine and topaz. The government provided the essential infrastructure such as good harbours, lighthouses, and warehouses to promote overseas trade. \n",
        "\n",
        "The trade route taken by ships from Rome to Tamilakam has been described in detail by the writers, such as Strabo and Pliny the Elder. Roman and Arab sailors were aware of the existence of the monsoon winds that blew across the Indian Ocean on a seasonal basis. A Roman captain named Hippalus first sailed a direct route from Rome to India, using the monsoon winds.\n",
        "\n",
        "Source: https://en.wikipedia.org/wiki/Economy_of_ancient_Tamil_country\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Character Splitting\n",
        "\n",
        "Character splitting is the most basic form of splitting up your text. It is the process of simply dividing your text into N-character sized chunks regardless of their content or form.\n",
        "\n",
        "This method isn't recommended for any applications - but it's a great starting point for us to understand the basics.\n",
        "\n",
        "**Pros**: Easy & Simple\n",
        "\n",
        "**Cons**: Very rigid and doesn't take into account the structure of your text\n",
        "Concepts to know:\n",
        "\n",
        "`Chunk Size` - The number of characters you would like in your chunks. 50, 100, 100,000, etc.\n",
        "\n",
        "`Chunk Overlap` - The amount you would like your sequential chunks to overlap. This is to try to avoid cutting a single piece of context into multiple pieces. This will create duplicate data across chunks.\n",
        "\n",
        "`strip_whitespace=False` - If you would like to retain whitespace from the beginning and end of your chunks. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    chunk_size=128, chunk_overlap=0, separator=\"\", strip_whitespace=False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "text_splitter.create_documents([sample_txt])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Observerations**: We are not taking into account the structure of our document at all. We simply split by a fixed number of characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recursive Character Text Splitting\n",
        "\n",
        "Split a text into chunks using a Text Splitter. \n",
        "\n",
        "Parameters include:\n",
        "\n",
        "`chunk_size`: Max size of the resulting chunks (in either characters or tokens, as selected)\n",
        "`chunk_overlap`: Overlap between the resulting chunks (in either characters or tokens, as selected)\n",
        "`length_function`: How to measure lengths of chunks, examples are included for either characters or tokens\n",
        "\n",
        "\n",
        "You can see the default separators for LangChain here. Let's take a look at them one by one.\n",
        "\n",
        "- `\"\\n\\n\"` - Double new line, or most commonly paragraph breaks\n",
        "- `\"\\n\"` - New lines\n",
        "- `\" \"` - Spaces\n",
        "- `\"\"` - Characters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "length_function = len\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    chunk_size=128,\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    chunk_overlap=1,\n",
        "\n",
        "    length_function=length_function,\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_splitter.create_documents([sample_txt])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's view this visually,\n",
        "\n",
        "<img src=\"images/chunk_visualization.png\" width=50%>\n",
        "\n",
        "<small>Source: https://chunkviz.up.railway.app/</small>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Document Specific Splitting\n",
        "\n",
        "This level is all about making your chunking strategy fit your different data formats.\n",
        " \n",
        "**Markdown Document splitter**\n",
        "\n",
        "Separators:\n",
        "\n",
        "- `\\n#{1,6}` - Split by new lines followed by a header (H1 through H6)\n",
        "- ```` ```\\\\n ```` - Code blocks\n",
        "- `\\n\\\\*\\\\*\\\\*+\\n `- Horizontal Lines\n",
        "- `\\n---+\\n` - Horizontal Lines\n",
        "- `\\n___+\\n` - Horizontal Lines\n",
        "- `\\n\\n` Double new lines\n",
        "- `\\n` - New line\n",
        "- `\" \"` - Spaces\n",
        "- `\"\"` - Character\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "markdown_text = \"\"\"\n",
        "# Trade Relationship Between Tamilakam and Rome: A Historical Overview\n",
        "\n",
        "During ancient times, **Tamilakam's** economy thrived due to its extensive trading relationships with Rome. Archaeological, literary, and numismatic evidence affirms the exchange of valuable commodities like spices, pearls, and ivory.\n",
        "\n",
        "## Key Commodities:\n",
        "- **Exports**: Pepper, Pearls, Ivory, Textiles, Gold Ornaments\n",
        "- **Imports**: Glass, Coral, Wine, Topaz\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import MarkdownTextSplitter\n",
        "\n",
        "splitter = MarkdownTextSplitter(chunk_size=40, chunk_overlap=0)\n",
        "\n",
        "md_splitter = splitter.create_documents([markdown_text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "md_splitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why Chunk Size Matters in Language Modeling (LLM)\n",
        "\n",
        "### Importance of Chunk Size\n",
        "Consider an article, where the initial sentences introduce entities by their names, while the latter ones rely solely on pronouns to reference them. The split chunks that don’t contain the actual entity names will lose the semantic meaning and won’t be retrieved through vector search. Therefore, replacing the pronouns with actual names can improve the semantic significance of split chunks in this case. Choosing the right chunk_size is a critical decision that can influence the efficiency and accuracy of a the system in several ways. Below are reasons why:\n",
        "\n",
        "- **Relevance and Granularity:** A small `chunk_size`, like `128`, yields more granular chunks. This granularity, however, presents a risk: vital information might not be among the top retrieved chunks, especially if the `similarity_top_k` setting is as restrictive as `2`. Conversely, a chunk size of 512 is likely to encompass all necessary information within the top chunks, ensuring that answers to queries are readily available. To navigate this, we employ the Faithfulness and Relevancy metrics. These measure the absence of ‘hallucinations’ and the ‘relevancy’ of responses based on the query and the retrieved contexts respectively.\n",
        "- **Response Generation Time:** As the `chunk_size` increases, so does the volume of information directed into the LLM to generate an answer. While this can ensure a more comprehensive context, it might also slow down the system. Ensuring that the added depth doesn't compromise the system's responsiveness is crucial.\n",
        "-. **Optimizing Response Generation Time**: With increasing `chunk_size`, the amount of data processed by the LLM for generating answers grows accordingly. Though providing a broader context, it may lead to longer response times. Keeping a reasonable equilibrium between depth and swiftness is indispensable.\n",
        "\n",
        "### Determining the Ideal Chunk Size\n",
        "To identify the most effective `chunk_size` for your particular application and dataset, rigorous testing across diverse sizes is mandatory. Moreover, tailor the `chunk_size` to individual stages in your pipeline. For instance, employ a larger `chunk_size` for high-level tasks like summarization, contrastingly, smaller `chunk_sizes` for low-level tasks like coding based on a function definition."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vector stores\n",
        "\n",
        "`Chroma` is a AI-native open-source vector database focused on developer productivity and happiness. Chroma is licensed under Apache 2.0. It is a simple and fast vector store that can be used to store and retrieve vectors.\n",
        "\n",
        "<small> Learn more about [ChromaDB](https://python.langchain.com/docs/integrations/vectorstores/chroma)  </small>\n",
        "\n",
        "<img src=\"images/langchain_retrieval.jpg\" width=40% />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\n",
        "    \"./../../datasets/raw_data/txt/2023_msft_earnings_call_transcript.txt\"\n",
        ")\n",
        "raw_data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(raw_data[0].page_content[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "length_function = len\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    chunk_size=128,\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    chunk_overlap=1,\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    length_function=length_function,\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "documents = text_splitter.split_documents(raw_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Embedding Text Using OpenSource Models\n",
        "\n",
        "_Note: You can also use OpenAI's GPT-3 for this task, but it is a paid service._\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "\n",
        "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# For OpenAI Embeddings\n",
        "# from langchain_openai import OpenAIEmbeddings\n",
        "# embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating Vector Store with Chroma DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "db = Chroma.from_documents(documents, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrievers\n",
        "\n",
        "Retrieving Semantically Similar Documents from vector stores\n",
        "\n",
        "<img src=\"images/langchain_retrieval.jpg\" width=40%/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"Revenue for last year?\"\n",
        "matching_docs = db.similarity_search(query)\n",
        "\n",
        "len(matching_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "matching_docs[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever = db.as_retriever()\n",
        "query = \"What did Satya say about growth?\"\n",
        "\n",
        "matching_docs = docs = retriever.get_relevant_documents(query)\n",
        "\n",
        "\n",
        "for doc in matching_docs:\n",
        "    print(doc.page_content)\n",
        "    print(doc.metadata)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Similarity score threshold retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.4}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs = retriever.get_relevant_documents(query)\n",
        "for doc in docs:\n",
        "    print(doc.page_content)\n",
        "    print(doc.metadata)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Maximum marginal relevance(mmr) retrieval\n",
        "\n",
        "The Max Marginal Relevance Example Selector selects examples based on a combination of which examples are most similar to the inputs, while also optimizing for diversity. It does this by finding the examples with the embeddings that have the greatest cosine similarity with the inputs, and then iteratively adding them while penalizing them for closeness to already selected examples.\n",
        "\n",
        "<Small> Additional Learning: [Simple Unsupervised Keyphrase Extraction using Sentence Embeddings](https://arxiv.org/pdf/1801.04470.pdf) </small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever = db.as_retriever(search_type=\"mmr\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs = retriever.get_relevant_documents(query)\n",
        "for doc in docs:\n",
        "    print(doc.page_content)\n",
        "    print(doc.metadata)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Retrieval top k results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever = db.as_retriever(search_kwargs={\"k\": 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs = retriever.get_relevant_documents(\n",
        "    \"what did he say about ketanji brown jackson\")\n",
        "\n",
        "\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Reading\n",
        "- [LangChain Retrieval](https://python.langchain.com/docs/modules/data_connection/)\n",
        "- [Langchain VectorDB](https://python.langchain.com/docs/integrations/vectorstores/chroma)\n",
        "- [Simple Unsupervised Keyphrase Extraction using Sentence Embedding](https://arxiv.org/pdf/1801.04470.pdf)\n",
        "- [Chunk Visualization](https://chunkviz.up.railway.app/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyONM96f7/m0jUCD9c87+MQy",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
