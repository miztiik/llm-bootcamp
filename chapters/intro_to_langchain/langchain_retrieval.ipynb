{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# Langchain Retrieval\n",
        "\n",
        "- Document Loaders\n",
        "- Text Splitting\n",
        "- Vector stores\n",
        "- Retreivers\n",
        "- few more tools..\n",
        "  \n",
        "<img src=\"images/langchain_retrieval.jpg\" width=75%/>\n",
        "\n",
        "**References**\n",
        "- [LangChain Retrieval](https://python.langchain.com/docs/modules/data_connection/)\n",
        "\n",
        "In this notebook, we will use the `langchain` library to use pre-trained models for various NLP tasks. \n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/miztiik/llm-bootcamp/blob/main/chapters/intro_to_langchain/langchain_retreivers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Comment the above line to see the installation logs\n",
        "\n",
        "# Install the dependencies\n",
        "!pip install -qU python-dotenv\n",
        "!pip install -qU langchain\n",
        "!pip install -qU langchain-openai\n",
        "!pip install -qU pypdf\n",
        "!pip install -qU unstructured\n",
        "!pip install -qU unstructured[md]\"\n",
        "!pip install -qU rapidocr-onnxruntime\n",
        "!pip install -qU chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -qU unstructured[md]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50dvxjqCFmhF"
      },
      "outputs": [],
      "source": [
        "# Load environment variables\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7RnyUOCJWmk"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")\n",
        "llm_chat = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Document loaders\n",
        "\n",
        "Download any reasonably sized pdf and upload it to the colab environment. We will use the `langchain` library to load the document and extract the text from it. You can find some samples in the `datasets` folder in the repo.\n",
        "\n",
        "<img src=\"images/langchain_retrieval.jpg\" width=40%/>\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\n",
        "    \"./../../datasets/raw_data/pdf/2023_india_economic_survey.pdf\")\n",
        "pages = loader.load_and_split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets check out the few pages from the document. An advantage of this approach is that documents can be retrieved with page numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pages[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extract text from images in PDF\n",
        "\n",
        "Using the `rapidocr-onnxruntime` package we can extract images as text as well:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -qU rapidocr-onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(\n",
        "    \"https://arxiv.org/pdf/2103.15348.pdf\", extract_images=True)\n",
        "pages = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pages[4].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loader = PyPDFLoader(\n",
        "    \"./../../datasets/raw_data/pdf/bain_on_strategy.pdf\", extract_images=True)\n",
        "pages = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pages[3].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading all documents in a folder based on extension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "loader = DirectoryLoader(\"./../../datasets/raw_data/\", glob=\"**/*.md\")\n",
        "docs = loader.load()\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
        "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
        "\n",
        "# For Markdown\n",
        "loader = UnstructuredMarkdownLoader(\"README.md\")\n",
        "# For HTML\n",
        "loader = UnstructuredHTMLLoader(\"index.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Splitters\n",
        "\n",
        "- **Character Splitting** - Simple static character chunks of data\n",
        "  - The problem with it is that we do not take into account the structure of our document at all. We simply split by a fixed number of characters.\n",
        "- **Recursive Character Text Splitting** - Recursive chunking based on a list of separators. We can specify a list of separators. This is the swiss army knife of splitters and my first choice, when you don't know which splitter to start with, this is a good first bet.\n",
        "- **Document Specific Splitting** - Various chunking methods for different document types (PDF, Python, Markdown)\n",
        "\n",
        "\n",
        "Additional Reading - [5 Levels Of Text Splitting](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/5_Levels_Of_Text_Splitting.ipynb)\n",
        "\n",
        "\n",
        "Lets try these on some sample text,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_txt = \"\"\"\n",
        "The economic prosperity of the Tamils depended on foreign trade. Literary, archaeological and numismatic sources confirm the trade relationship between Tamilakam and Rome, where spices and pearls from India were in great demand. With the accession of Augustus in 27 BCE, trade between Tamilakam and Rome received a tremendous boost and culminated at the time of Nero who died in 68 CE. At that point, trade declined until the death of Caracalla (217 CE), after which it almost ceased. It was revived again under the Byzantine emperors. Under the early Roman emperors, there was a great demand for articles of luxury, especially beryl. \n",
        "\n",
        "Most of the articles of luxury mentioned by the Roman writers came from Tamilakam. In the declining period, cotton and industrial products were still imported by Rome. The exports from the Tamil country included pepper, pearls, ivory, textiles and gold ornaments, while the imports were luxury goods such as glass, coral, wine and topaz. The government provided the essential infrastructure such as good harbours, lighthouses, and warehouses to promote overseas trade. \n",
        "\n",
        "The trade route taken by ships from Rome to Tamilakam has been described in detail by the writers, such as Strabo and Pliny the Elder. Roman and Arab sailors were aware of the existence of the monsoon winds that blew across the Indian Ocean on a seasonal basis. A Roman captain named Hippalus first sailed a direct route from Rome to India, using the monsoon winds.\n",
        "\n",
        "Source: https://en.wikipedia.org/wiki/Economy_of_ancient_Tamil_country\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Character Splitting\n",
        "\n",
        "Character splitting is the most basic form of splitting up your text. It is the process of simply dividing your text into N-character sized chunks regardless of their content or form.\n",
        "\n",
        "This method isn't recommended for any applications - but it's a great starting point for us to understand the basics.\n",
        "\n",
        "**Pros**: Easy & Simple\n",
        "\n",
        "**Cons**: Very rigid and doesn't take into account the structure of your text\n",
        "Concepts to know:\n",
        "\n",
        "`Chunk Size` - The number of characters you would like in your chunks. 50, 100, 100,000, etc.\n",
        "\n",
        "`Chunk Overlap` - The amount you would like your sequential chunks to overlap. This is to try to avoid cutting a single piece of context into multiple pieces. This will create duplicate data across chunks.\n",
        "\n",
        "`strip_whitespace=False` - If you would like to retain whitespace from the beginning and end of your chunks. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    chunk_size=128,\n",
        "    chunk_overlap=0,\n",
        "    separator='',\n",
        "    strip_whitespace=False\n",
        ")\n",
        "\n",
        "\n",
        "text_splitter.create_documents([sample_txt])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Observerations**: We are not taking into account the structure of our document at all. We simply split by a fixed number of characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recursive Character Text Splitting\n",
        "\n",
        "Split a text into chunks using a Text Splitter. \n",
        "\n",
        "Parameters include:\n",
        "\n",
        "`chunk_size`: Max size of the resulting chunks (in either characters or tokens, as selected)\n",
        "`chunk_overlap`: Overlap between the resulting chunks (in either characters or tokens, as selected)\n",
        "`length_function`: How to measure lengths of chunks, examples are included for either characters or tokens\n",
        "\n",
        "\n",
        "You can see the default separators for LangChain here. Let's take a look at them one by one.\n",
        "\n",
        "- `\"\\n\\n\"` - Double new line, or most commonly paragraph breaks\n",
        "- `\"\\n\"` - New lines\n",
        "- `\" \"` - Spaces\n",
        "- `\"\"` - Characters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "length_function = len\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
        "    chunk_size=128,\n",
        "    chunk_overlap=1,\n",
        "    length_function=length_function,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_splitter.create_documents([sample_txt])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's view this visually,\n",
        "\n",
        "<img src=\"images/chunk_visualization.png\" width=50%>\n",
        "\n",
        "<small>Source: https://chunkviz.up.railway.app/</small>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Document Specific Splitting\n",
        "\n",
        "This level is all about making your chunking strategy fit your different data formats.\n",
        " \n",
        "**Markdown Document splitter**\n",
        "\n",
        "Separators:\n",
        "\n",
        "- `\\n#{1,6}` - Split by new lines followed by a header (H1 through H6)\n",
        "- ```` ```\\\\n ```` - Code blocks\n",
        "- `\\n\\\\*\\\\*\\\\*+\\n `- Horizontal Lines\n",
        "- `\\n---+\\n` - Horizontal Lines\n",
        "- `\\n___+\\n` - Horizontal Lines\n",
        "- `\\n\\n` Double new lines\n",
        "- `\\n` - New line\n",
        "- `\" \"` - Spaces\n",
        "- `\"\"` - Character\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "markdown_text = \"\"\"\n",
        "# Trade Relationship Between Tamilakam and Rome: A Historical Overview\n",
        "\n",
        "During ancient times, **Tamilakam's** economy thrived due to its extensive trading relationships with Rome. Archaeological, literary, and numismatic evidence affirms the exchange of valuable commodities like spices, pearls, and ivory.\n",
        "\n",
        "## Key Commodities:\n",
        "- **Exports**: Pepper, Pearls, Ivory, Textiles, Gold Ornaments\n",
        "- **Imports**: Glass, Coral, Wine, Topaz\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import MarkdownTextSplitter\n",
        "\n",
        "splitter = MarkdownTextSplitter(chunk_size=40, chunk_overlap=0)\n",
        "\n",
        "md_splitter = splitter.create_documents([markdown_text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(page_content='# Trade Relationship Between Tamilakam'),\n",
              " Document(page_content='and Rome: A Historical Overview'),\n",
              " Document(page_content=\"During ancient times, **Tamilakam's**\"),\n",
              " Document(page_content='economy thrived due to its extensive'),\n",
              " Document(page_content='trading relationships with Rome.'),\n",
              " Document(page_content='Archaeological, literary, and'),\n",
              " Document(page_content='numismatic evidence affirms the'),\n",
              " Document(page_content='exchange of valuable commodities like'),\n",
              " Document(page_content='spices, pearls, and ivory.'),\n",
              " Document(page_content='## Key Commodities:'),\n",
              " Document(page_content='- **Exports**: Pepper, Pearls, Ivory,'),\n",
              " Document(page_content='Textiles, Gold Ornaments'),\n",
              " Document(page_content='- **Imports**: Glass, Coral, Wine,'),\n",
              " Document(page_content='Topaz')]"
            ]
          },
          "execution_count": 161,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "md_splitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Conclusion\n",
        "\n",
        "- **Chunk Size**: A  very typical case is that the chunks may lose information when split up. Consider a typical article, where the initial sentences introduce entities by their names, while the latter ones rely solely on pronouns to reference them. The split chunks that don’t contain the actual entity names will lose the semantic meaning and won’t be retrieved through vector search. Therefore, replacing the pronouns with actual names can improve the semantic significance of split chunks in this case.\n",
        "  - Chunk size based on usecase - You do not have to stick to one chunk optimization method for all the steps in your pipeline. For example, if your pipeline involves both high-level tasks like summarization and low-level tasks like coding based on a function definition, you could try to use a bigger chunk size for summarization and then smaller chunks for coding reference.\n",
        "- "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vector stores\n",
        "\n",
        "<img src=\"images/langchain_retrieval.jpg\" width=40% />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\n",
        "    \"./../../datasets/raw_data/txt/2023_msft_earnings_call_transcript.txt\")\n",
        "raw_data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source:\n",
            "https://www.fool.com/earnings/call-transcripts/2023/10/24/microsoft-msft-q1-2024-earnings-call-transcript/\n",
            "https://www.fool.com/earnings/call-transcripts/2024/01/31/microsoft-msft-q2-2024-earnings-call-transcript/\n",
            "https://www.microsoft.com/en-us/Investor/earnings/FY-2023-Q3/press-release-webcast\n",
            "https://www.fool.com/earnings/call-transcripts/2023/07/25/microsoft-msft-q4-2023-earnings-call-transcript/\n",
            "================================================================\n",
            "Microsoft FY23 Third Quarter Earnings Conference Call\n",
            "Brett Iversen, Satya Nadella, Amy Hood\n",
            "Tuesday, April 25, 2023\n",
            "\n",
            "BRETT IVERSEN: \n",
            "\n",
            "Good afternoon and thank you for joining us today. On the call with me are Satya Nadella, chairman and chief executive officer, Amy Hood, chief financial officer, Alice Jolla, chief accounting officer, and Keith Dolliver, deputy general counsel.\n",
            "\n",
            "On the Microsoft Investor Relations website, you can find our earnings press release and financial summary slide deck, which is intended to s\n"
          ]
        }
      ],
      "source": [
        "print(raw_data[0].page_content[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "length_function = len\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
        "    chunk_size=128,\n",
        "    chunk_overlap=1,\n",
        "    length_function=length_function,\n",
        ")\n",
        "\n",
        "documents = text_splitter.split_documents(raw_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Embedding Text Using Langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "\n",
        "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating Vector Store with Chroma DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [],
      "source": [
        "db = Chroma.from_documents(documents, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrievers\n",
        "\n",
        "Retrieving Semantically Similar Documents from vector stores\n",
        "\n",
        "<img src=\"images/langchain_retrieval.jpg\" width=40%/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 163,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"Revenue for last year?\"\n",
        "matching_docs = db.similarity_search(query)\n",
        "\n",
        "len(matching_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='revenue in the next 12 months, up 15% year over year. The remaining portion, which will be recognized beyond the next 12', metadata={'source': './../../datasets/raw_data/txt/2023_msft_earnings_call_transcript.txt'})"
            ]
          },
          "execution_count": 152,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "matching_docs[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In addition, what Satya mentioned earlier in a question, and I just want to take every chance to reiterate it, if you have a\n",
            "{'source': './../../datasets/raw_data/txt/2024_fy_msft_investor_earnings_call_q1_and_q2.txt'}\n",
            "\n",
            "\n",
            "In addition, what Satya mentioned earlier in a question, and I just want to take every chance to reiterate it, if you have a\n",
            "{'source': './../../datasets/raw_data/txt/2024_fy_msft_investor_earnings_call_q1_and_q2.txt'}\n",
            "\n",
            "\n",
            "In addition, what Satya mentioned earlier in a question, and I just want to take every chance to reiterate it, if you have a\n",
            "{'source': './../../datasets/raw_data/txt/2023_msft_earnings_call_transcript.txt'}\n",
            "\n",
            "\n",
            "In addition, what Satya mentioned earlier in a question, and I just want to take every chance to reiterate it, if you have a\n",
            "{'source': './../../datasets/raw_data/txt/2023_msft_earnings_call_transcript.txt'}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "retriever = db.as_retriever()\n",
        "query = \"What did Satya say about growth?\"\n",
        "\n",
        "matching_docs = docs = retriever.get_relevant_documents(query)\n",
        "\n",
        "\n",
        "for doc in matching_docs:\n",
        "    print(doc.page_content)\n",
        "    print(doc.metadata)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Similarity score threshold retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.4}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In addition, what Satya mentioned earlier in a question, and I just want to take every chance to reiterate it, if you have a\n",
            "{'source': './../../datasets/raw_data/txt/2024_fy_msft_investor_earnings_call_q1_and_q2.txt'}\n",
            "\n",
            "\n",
            "In addition, what Satya mentioned earlier in a question, and I just want to take every chance to reiterate it, if you have a\n",
            "{'source': './../../datasets/raw_data/txt/2024_fy_msft_investor_earnings_call_q1_and_q2.txt'}\n",
            "\n",
            "\n",
            "In addition, what Satya mentioned earlier in a question, and I just want to take every chance to reiterate it, if you have a\n",
            "{'source': './../../datasets/raw_data/txt/2023_msft_earnings_call_transcript.txt'}\n",
            "\n",
            "\n",
            "In addition, what Satya mentioned earlier in a question, and I just want to take every chance to reiterate it, if you have a\n",
            "{'source': './../../datasets/raw_data/txt/2023_msft_earnings_call_transcript.txt'}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "docs = retriever.get_relevant_documents(query)\n",
        "for doc in docs:\n",
        "    print(doc.page_content)\n",
        "    print(doc.metadata)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Maximum marginal relevance retrieval\n",
        "\n",
        "MMR tries to reduce the redundancy of results while at the same time maintaining query relevance of results for already ranked documents/phrases etc.\n",
        "\n",
        "<Small> Additional Learning: [Simple Unsupervised Keyphrase Extraction using Sentence Embeddings](https://arxiv.org/pdf/1801.04470.pdf) </small>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever = db.as_retriever(search_type=\"mmr\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In addition, what Satya mentioned earlier in a question, and I just want to take every chance to reiterate it, if you have a\n",
            "{'source': './../../datasets/raw_data/txt/2024_fy_msft_investor_earnings_call_q1_and_q2.txt'}\n",
            "\n",
            "\n",
            "In addition, what Satya mentioned earlier in a question, and I just want to take every chance to reiterate it, if you have a\n",
            "{'source': './../../datasets/raw_data/txt/2024_fy_msft_investor_earnings_call_q1_and_q2.txt'}\n",
            "\n",
            "\n",
            "In addition, what Satya mentioned earlier in a question, and I just want to take every chance to reiterate it, if you have a\n",
            "{'source': './../../datasets/raw_data/txt/2023_msft_earnings_call_transcript.txt'}\n",
            "\n",
            "\n",
            "In addition, what Satya mentioned earlier in a question, and I just want to take every chance to reiterate it, if you have a\n",
            "{'source': './../../datasets/raw_data/txt/2023_msft_earnings_call_transcript.txt'}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "docs = retriever.get_relevant_documents(query)\n",
        "for doc in docs:\n",
        "    print(doc.page_content)\n",
        "    print(doc.metadata)\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Retrieval top k results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {},
      "outputs": [],
      "source": [
        "retriever = db.as_retriever(search_kwargs={\"k\": 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 200,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs = retriever.get_relevant_documents(\n",
        "    \"what did he say about ketanji brown jackson\")\n",
        "len(docs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyONM96f7/m0jUCD9c87+MQy",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
