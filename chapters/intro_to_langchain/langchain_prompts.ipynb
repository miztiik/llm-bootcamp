{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# Langchain Prompts\n",
        "\n",
        "A prompt for a language model is a set of instructions or input provided by a user to guide the model's response, helping it understand the context and generate relevant and coherent language-based output, such as answering questions, completing sentences, or engaging in a conversation.\n",
        "\n",
        "- String prompt template\n",
        "- Chat prompt templates\n",
        "\n",
        "In this notebook, we will use the `langchain` library to use pre-trained models for various NLP tasks.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/miztiik/llm-bootcamp/blob/main/chapters/intro_to_langchain/langchain_prompts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Comment the above line to see the installation logs\n",
        "\n",
        "# Install the dependencies\n",
        "!pip install -qU python-dotenv\n",
        "!pip install -qU langchain\n",
        "!pip install -qU langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50dvxjqCFmhF"
      },
      "outputs": [],
      "source": [
        "# Load environment variables\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## String prompt template\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")\n",
        "llm_chat = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=0.3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Tell me a {adjective} joke about {content}.\"\n",
        ")\n",
        "prompt_template.format(adjective=\"funny\", content=\"AI\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "joke_resp = llm.invoke(prompt_template.format(\n",
        "    adjective=\"sad\", content=\"Tech Engineer\"))\n",
        "\n",
        "print(joke_resp, end=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "template = \"\"\"\n",
        "You are an expert data scientist with an expertise in building deep learning models. \n",
        "Explain the concept of {concept} in a couple of lines.\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"concept\"],\n",
        "    template=template,\n",
        "    template_format='f-string',\n",
        "    validate_template=True\n",
        ")\n",
        "\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run LLM with PromptTemplate\n",
        "prompt_template_resp = llm.invoke(prompt.format(concept=\"generative models\"))\n",
        "print(prompt_template_resp, end=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try another query with PromptTemplate\n",
        "prompt_template_resp = llm.invoke(\n",
        "    prompt.format(concept=\"Large Language Models\"))\n",
        "print(prompt_template_resp, end=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chat prompt composition\n",
        "\n",
        "Source: \n",
        "- https://python.langchain.com/docs/modules/model_io/chat/quick_start#messages\n",
        "- https://python.langchain.com/docs/modules/model_io/prompts/composition#chat-prompt-composition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        # SystemMessage(content=(\"You are a {sports} historian.\")),\n",
        "        SystemMessagePromptTemplate.from_template(\n",
        "            \"You are a {sports} historian.\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chat_prompt = chat_template.format_prompt(\n",
        "    sports=\"Tennis\",\n",
        "    text=\"Who won the Australian Open in 2015\").to_messages()\n",
        "\n",
        "print(chat_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_resp = llm_chat.invoke(chat_prompt)\n",
        "print(prompt_resp.content, end=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try another query with ChatPromptTemplate\n",
        "print(llm_chat.invoke(\n",
        "    chat_template.format_prompt(\n",
        "        sports=\"Football\", text=\"Who won the 2018 FIFA World Cup?\").to_messages()\n",
        ")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Reading\n",
        "\n",
        "- [LangChain Prompts](https://python.langchain.com/docs/modules/model_io/prompts/)\n",
        "- [LangChain Quick Start](https://python.langchain.com/docs/modules/model_io/prompts/quick_start)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyONM96f7/m0jUCD9c87+MQy",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
