{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Tokenization is the process of breaking down a text into smaller units called tokens. These tokens can be individual words, punctuation marks, or even subwords (e.g., prefixes, suffixes, or roots). Tokenization helps in converting continuous text into a discrete form that can be easily processed by machine learning algorithms.\n",
    "\n",
    "There are different methods for tokenization, including:\n",
    "\n",
    "- **Word-level tokenization:** This method splits the text into individual words, treating each word as a separate token.\n",
    "- **Character-level tokenization:** This method breaks down the text into individual characters, considering each character as a separate token.\n",
    "- **Subword tokenization:** This method divides the text into smaller units, such as prefixes, suffixes, or roots, which can be combined to form words. This is particularly useful for dealing with rare or out-of-vocabulary words.\n",
    "\n",
    "The tokenization process is done by the `tokenize()` method of the tokenizer:\n",
    "\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/miztiik/llm-bootcamp/blob/main/chapters/tokenization_vs_embeddings/tokenization_vs_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "<img src=\"./images/tokenize.png\" width=50%>\n",
    "\n",
    "In this notebook, we will explore different tokenization methods using the `transformers` library. We will use the `BertTokenizer` class to tokenize a given text and visualize the tokens generated by different tokenization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Comment the above line to see the installation logs\n",
    "\n",
    "# Install the latest version of the transformers library\n",
    "!pip install -qU transformers\n",
    "!pip install -qU torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['using', 'a', 'transform', '##er', 'model', 'is', 'simple']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(\"Using a Transformer Model is simple\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tokenizer is a subword tokenizer: it splits the words until it obtains tokens that can be represented by its vocabulary. Thatâ€™s the case here with `transformer`, which is split into two tokens: `transform` and `##er`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From tokens to input IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2478, 1037, 10938, 2121, 2944, 2003, 3722]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding the input IDs\n",
    "\n",
    "<img src=\"./images/tokenize_encode_decode.png\" width=50%>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using a transformer model is simple\n"
     ]
    }
   ],
   "source": [
    "decoded_string = tokenizer.decode(ids)\n",
    "# decoded_string = tokenizer.decode([7993, 170, 13809, 23763, 6747, 1110, 3014])\n",
    "\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 6747, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Using a Transformer Model is simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Embedding is the process of converting tokens into dense numerical vectors that capture the semantic relationships between words. These vectors, known as word embeddings, allow machines to understand and analyze the meaning and context of words in a text.\n",
    "\n",
    "<img src=\"./images/text_embeddings.png\" width=50%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['using', 'a', 'transform', '##er', 'model', 'is', 'simple']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer.tokenize(\"Using a Transformer Model is simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: tensor([[  101,  2478,  1037, 10938,  2121,  2944,  2003,  3722,   102]])\n",
      "Decoded tokens: [CLS] using a transformer model is simple [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\n",
    "    \"Using a Transformer Model is simple\", return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Decoded tokens:\", tokenizer.decode(tokens[0]))\n",
    "\n",
    "# for token in tokens[0]:\n",
    "#     print(\"These are decoded tokens!\", tokenizer.decode([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "txt_embeddings = model.embeddings.word_embeddings(tokens)\n",
    "\n",
    "\n",
    "# for e in model.embeddings.word_embeddings(tokens)[0]:\n",
    "#     print(\"This is an embedding!\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 768])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "he output `torch.Size([1, 9, 768])` indicates the shape of the embeddings `txt_embeddings`. Here's an explanation of each dimension:\n",
    "\n",
    "`1` : This dimension corresponds to the batch size. Since we only have one sample in our case (a single sentence), the first dimension has a size of `1`.\n",
    "`9`: This dimension signifies the sequence length, which refers to the number of tokens in the input sequence including any padding sequences added by tokenizer.\n",
    "`768`: Lastly, this dimension shows the size of each individual embedding vector. Each token in the input sequence receives an embedding representation, and these representations form the third dimension. In this example, the `DistilBERT` model produces embeddings of size `768` for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0390, -0.0123, -0.0208,  ...,  0.0607,  0.0230,  0.0238],\n",
       "         [-0.0591,  0.0156, -0.0018,  ...,  0.0122,  0.0091, -0.0047],\n",
       "         [ 0.0062,  0.0100,  0.0071,  ..., -0.0043, -0.0132,  0.0166],\n",
       "         ...,\n",
       "         [-0.0440, -0.0236, -0.0283,  ...,  0.0053, -0.0081,  0.0170],\n",
       "         [ 0.0253, -0.0128, -0.0227,  ..., -0.0412, -0.0077, -0.0559],\n",
       "         [-0.0199, -0.0095, -0.0099,  ..., -0.0235,  0.0071, -0.0071]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt_embeddings[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(txt_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Reading\n",
    "\n",
    "- [The amazing power of word vectors](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
