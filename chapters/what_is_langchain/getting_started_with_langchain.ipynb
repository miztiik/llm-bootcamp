{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# What is Langchain\n",
        "Langchain is an open-source framework enabling developers to integrate large language models like GPT-4 with external computation and data sources. It is available as a Python or JavaScript/TypeScript package.\n",
        "\n",
        "## Why Langchain is needed:\n",
        "Langchain is essential for connecting large language models to personalized data sources, such as books, PDFs, or databases, allowing users to interact with their own information dynamically. It facilitates the creation of data-aware and authentic applications, opening up various practical use cases, including personal assistance, learning, coding, data analysis, and connecting language models to company data for advanced analytics. The framework's main value proposition lies in LLW wrappers, prompt templates, chains, and agents, enabling seamless integration and interaction with language models.\n",
        "\n",
        "**References**\n",
        "- [LangChain](https://python.langchain.com/docs/get_started/quickstart)\n",
        "\n",
        "In this notebook, we will use the `langchain` library to use pre-trained models for various NLP tasks. We will use the `pipeline` class to use pre-trained models for various NLP tasks. The `pipeline` class provides a simple API dedicated to several NLP tasks. It provides a simple, straight-forward, and efficient way to use pre-trained models.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/miztiik/llm-bootcamp/blob/main/chapters/what_is_langchain/getting_started_with_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Comment the above line to see the installation logs\n",
        "\n",
        "# Install the dependencies\n",
        "!pip install -qU python-dotenv\n",
        "!pip install -qU langchain\n",
        "!pip install -qU langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "50dvxjqCFmhF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load environment variables\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "\n",
        "load_dotenv(find_dotenv())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [
          "Models"
        ]
      },
      "source": [
        "## LangChain: Models - Text Model Wrapper\n",
        "\n",
        "Update your `OPENAI_API_KEY` in the `.env` file to use the OpenAI model. You can get the API key from the [OpenAI website](https://platform.openai.com/account/api-keys)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "C7RnyUOCJWmk"
      },
      "outputs": [],
      "source": [
        "# Run basic query with OpenAI wrapper\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "llm = OpenAI()\n",
        "\n",
        "# To specify a particular model refer to the OpenAI documentation - https://platform.openai.com/docs/models\n",
        "# llm = OpenAI(model_name=\"gpt-3.5-turbo-0125\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\nThe currency of India is the Indian Rupee (INR).'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.invoke(\"What is the currency of india\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt_resp = llm.invoke(\"explain large language models in one sentence\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(txt_resp, end=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"You can use the `**` operator to square a given number in Python. Here's an example:\\n\\n```python\\ndef square_number(number):\\n    return number ** 2\\n\\n# Example usage\\nresult = square_number(5)\\nprint(result)  # Output: 25\\n```\\n\\nIn this example, the `square_number` function takes a number as input and returns the square of that number using the `**` operator.\")"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_chat = ChatOpenAI(temperature=0.3)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are an expert python programmer\"),\n",
        "    HumanMessage(content=\"Python method to square given number?\"),\n",
        "]\n",
        "\n",
        "chat_resp = llm_chat.invoke(messages)\n",
        "chat_resp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Langchain: Prompt Templates\n",
        "\n",
        "A prompt for a language model is a set of instructions or input provided by a user to guide the model's response, helping it understand the context and generate relevant and coherent language-based output, such as answering questions, completing sentences, or engaging in a conversation.\n",
        "\n",
        "- String prompt template\n",
        "- Chat prompt templates\n",
        "\n",
        "Source: https://python.langchain.com/docs/modules/model_io/prompts/quick_start\n",
        "\n",
        "### String Prompt Template\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Tell me a funny joke about AI.'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    \"Tell me a {adjective} joke about {content}.\"\n",
        ")\n",
        "prompt_template.format(adjective=\"funny\", content=\"AI\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "template = \"\"\"\n",
        "You are an expert data scientist with an expertise in building deep learning models. \n",
        "Explain the concept of {concept} in a couple of lines.\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"concept\"],\n",
        "    template=template,\n",
        "    template_format='f-string',\n",
        "    validate_template=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['concept'], template='\\nYou are an expert data scientist with an expertise in building deep learning models. \\nExplain the concept of {concept} in a couple of lines.\\n', validate_template=True)"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generative models are a type of deep learning model that can learn the underlying patterns and structures of a given dataset in order to generate new data that mimics the original data. This allows for the creation of new data samples that are similar to the original data, making generative models useful for tasks such as data augmentation and image generation.\n"
          ]
        }
      ],
      "source": [
        "# Run LLM with PromptTemplate\n",
        "prompt_template_resp = llm.invoke(prompt.format(concept=\"generative models\"))\n",
        "print(prompt_template_resp, end=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Large Language Models (LLMs) refer to deep learning models that are trained on a large amount of text data and can generate human-like text responses. These models use advanced techniques such as attention mechanisms and transformer architectures to understand the context of the given text and produce coherent and relevant responses. LLMs are often used in natural language processing tasks such as text completion, question-answering, and text summarization.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "prompt_template_resp = llm.invoke(\n",
        "    prompt.format(concept=\"Large Language Models\"))\n",
        "print(prompt_template_resp, end=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chat prompt composition\n",
        "\n",
        "Source: \n",
        "- https://python.langchain.com/docs/modules/model_io/chat/quick_start#messages\n",
        "- https://python.langchain.com/docs/modules/model_io/prompts/composition#chat-prompt-composition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtHgQ5XpJmgi"
      },
      "outputs": [],
      "source": [
        "# import schema for chat messages and ChatOpenAI in order to query chat models like GPT-3.5-turbo or GPT-4\n",
        "\n",
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "from langchain.chat_models import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrfYfKfdJyyF"
      },
      "outputs": [],
      "source": [
        "llm_chat = ChatOpenAI(temperature=0.3)\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are an expert in python programmer\"),\n",
        "    HumanMessage(content=\"Python method to square given number?\"),\n",
        "]\n",
        "\n",
        "chat_resp = llm_chat.invoke(messages)\n",
        "\n",
        "\n",
        "print(chat_resp.content, end=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make the output of the model more human-like,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You can use the `**` operator to square a given number in Python. Here's an example:\n",
            "\n",
            "```python\n",
            "def square_number(number):\n",
            "    return number ** 2\n",
            "\n",
            "# Example usage\n",
            "result = square_number(5)\n",
            "print(result)  # Output: 25\n",
            "```\n",
            "\n",
            "In this example, the `square_number` function takes a `number` as input and returns the square of that number using the `**` operator.\n"
          ]
        }
      ],
      "source": [
        "print(chat_resp.content, end=\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Stream the output from the chat model to the console."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To square a given number in Python, you can use the `**` operator or the `pow()` function. Here is an example using both approaches:\n",
            "\n",
            "```python\n",
            "def square_number(number):\n",
            "    # Using the ** operator\n",
            "    squared = number ** 2\n",
            "    print(f\"The square of {number} is {squared}\")\n",
            "\n",
            "    # Using the pow() function\n",
            "    squared = pow(number, 2)\n",
            "    print(f\"The square of {number} is {squared}\")\n",
            "\n",
            "# Example usage\n",
            "square_number(5)\n",
            "```\n",
            "\n",
            "Output:\n",
            "```\n",
            "The square of 5 is 25\n",
            "The square of 5 is 25\n",
            "```\n",
            "\n",
            "In this example, the `square_number()` function takes a `number` parameter and calculates the square using both the `**` operator and the `pow()` function. The result is then printed to the console."
          ]
        }
      ],
      "source": [
        "for chunk in llm_chat.stream(messages):\n",
        "    print(chunk.content, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Langchain: Chains\n",
        "\n",
        "We can also guide the response with a prompt template. _Prompt templates_ are used to convert raw `user input` to a `better input` to the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", \"You are an expert technical advisor.\"), (\"user\", \"{input}\")]\n",
        ")\n",
        "\n",
        "chain = prompt | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Sure, here's a Python script that generates a list of prime numbers up to 100:\\n\\n```python\\ndef is_prime(n):\\n    if n <= 1:\\n        return False\\n    for i in range(2, int(n**0.5) + 1):\\n        if n % i == 0:\\n            return False\\n    return True\\n\\nprimes = []\\nfor num in range(2, 101):\\n    if is_prime(num):\\n        primes.append(num)\\n\\nprint(primes)\\n```\\n\\nThis script defines a function `is_prime()` which checks if a given number is prime. It iterates from 2 to the square root of the number (inclusive) and if any divisor is found, it returns `False`. Otherwise, it returns `True`.\\n\\nThen, it loops through numbers from 2 to 100 and checks if each number is prime using the `is_prime()` function. If a number is prime, it is appended to the `primes` list.\\n\\nFinally, the script prints the list of prime numbers.\")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke(\n",
        "    {\"input\": \"Write a Python script to generate a list of prime numbers up to 100.?\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2grf7I8AJ_hK"
      },
      "outputs": [],
      "source": [
        "# Import prompt and define PromptTemplate\n",
        "\n",
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "You are an expert data scientist with an expertise in building deep learning models. \n",
        "Explain the concept of {concept} in a couple of lines\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"concept\"],\n",
        "    template=template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcz7Q9Y-KFvI"
      },
      "outputs": [],
      "source": [
        "# Run LLM with PromptTemplate\n",
        "\n",
        "llm(prompt.format(concept=\"autoencoder\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dm78i-rUKXIB"
      },
      "outputs": [],
      "source": [
        "# Import LLMChain and define chain with language model and prompt as arguments.\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Run the chain only specifying the input variable.\n",
        "print(chain.run(\"autoencoder\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6MF4-nMKul3"
      },
      "outputs": [],
      "source": [
        "# Define a second prompt\n",
        "\n",
        "second_prompt = PromptTemplate(\n",
        "    input_variables=[\"ml_concept\"],\n",
        "    template=\"Turn the concept description of {ml_concept} and explain it to me like I'm five in 500 words\",\n",
        ")\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkJKFyk1K-MO"
      },
      "outputs": [],
      "source": [
        "# Define a sequential chain using the two chains above: the second chain takes the output of the first chain as input\n",
        "\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
        "\n",
        "# Run the chain specifying only the input variable for the first chain.\n",
        "explanation = overall_chain.run(\"autoencoder\")\n",
        "print(explanation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDDu1B_SLQls"
      },
      "outputs": [],
      "source": [
        "# Import utility for splitting up texts and split up the explanation given above into document chunks\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=0,\n",
        ")\n",
        "\n",
        "texts = text_splitter.create_documents([explanation])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6lfAdeuLhtp"
      },
      "outputs": [],
      "source": [
        "# Individual text chunks can be accessed with \"page_content\"\n",
        "\n",
        "texts[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5sv4e3tLw2y"
      },
      "outputs": [],
      "source": [
        "# Import and instantiate OpenAI embeddings\n",
        "\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model_name=\"ada\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dqzoir4hMlfl"
      },
      "outputs": [],
      "source": [
        "# Turn the first text chunk into a vector with the embedding\n",
        "\n",
        "query_result = embeddings.embed_query(texts[0].page_content)\n",
        "print(query_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaOY5bIZM3Xz"
      },
      "outputs": [],
      "source": [
        "# Import and initialize Pinecone client\n",
        "\n",
        "import os\n",
        "import pinecone\n",
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "\n",
        "pinecone.init(\n",
        "    api_key=os.getenv(\"PINECONE_API_KEY\"), environment=os.getenv(\"PINECONE_ENV\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZhSUt3FNBzN"
      },
      "outputs": [],
      "source": [
        "# Upload vectors to Pinecone\n",
        "\n",
        "index_name = \"langchain-quickstart\"\n",
        "search = Pinecone.from_documents(texts, embeddings, index_name=index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCXVuXwPNKcc"
      },
      "outputs": [],
      "source": [
        "# Do a simple vector similarity search\n",
        "\n",
        "query = \"What is magical about an autoencoder?\"\n",
        "result = search.similarity_search(query)\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ogz8luZNRnJ"
      },
      "outputs": [],
      "source": [
        "# Import Python REPL tool and instantiate Python agent\n",
        "\n",
        "from langchain.agents.agent_toolkits import create_python_agent\n",
        "from langchain.tools.python.tool import PythonREPLTool\n",
        "from langchain.python import PythonREPL\n",
        "from langchain.llms.openai import OpenAI\n",
        "\n",
        "agent_executor = create_python_agent(\n",
        "    llm=OpenAI(temperature=0, max_tokens=1000), tool=PythonREPLTool(), verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVHMDj0sNi09"
      },
      "outputs": [],
      "source": [
        "# Execute the Python agent\n",
        "\n",
        "agent_executor.run(\n",
        "    \"Find the roots (zeros) if the quadratic function 3 * x**2 + 2*x -1\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyONM96f7/m0jUCD9c87+MQy",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
