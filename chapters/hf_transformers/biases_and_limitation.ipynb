{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and Limitations in Pretrained Models\n",
    "\n",
    "Pretrained models, while powerful and versatile, are not immune to biases and inherent limitations. These models are trained on vast datasets that may inadvertently contain biases present in the real world, perpetuating and sometimes amplifying existing societal prejudices. For instance, if a large portion of training data comes from one particular demographic group, the model may perform poorly when applied to other groups due to lack of representation in the training data.\n",
    "\n",
    "Inherent limitations, on the other hand, are fundamental constraints imposed by the nature of the model architecture or the underlying algorithms. These limitations include the inability to understand sarcasm, irony, or nuanced meanings in text, or the difficulty in handling out-of-distribution data.\n",
    "\n",
    "## Bias in Text Generation\n",
    "When asked to fill in the missing word in these two sentences, the model gives only one gender-free answer (waiter/waitress). The others are work occupations usually associated with one specific gender "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carpenter', 'lawyer', 'farmer', 'businessman', 'doctor']\n",
      "['nurse', 'maid', 'teacher', 'waitress', 'prostitute']\n",
      "['woman', 'man', 'girl', 'lady', 'secretary']\n",
      "['he', 'she', 'michael', 'scott', 'everyone']\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This woman works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"The CEO is a man. His secretary is a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"[MASK] plays golf during the week.\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "- **Biased Completions:**\n",
    "  - The model might frequently complete the prompts with stereotypical associations (e.g., \"woman\" for nurse, teacher, secretary).\n",
    "\n",
    "- **Reinforcing Stereotypes:**\n",
    "  - These completions can reflect societal biases present in the training data, perpetuating harmful stereotypes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias in Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9907229542732239}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_task = pipeline(\"sentiment-analysis\")\n",
    "sentiment_task(\"The phone has a long battery life.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "The word _long_ might be associated with negative reviews (e.g., \"takes a long time to charge\")\n",
    "\n",
    "**Need for Awareness:** It's crucial to recognize these biases and use pretrained models with caution, understanding their limitations and potential for perpetuating injustice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies for Ethical Development and Enhancement of Pretrained Models: Mitigating Biases and Overcoming Limitations\n",
    "\n",
    "To mitigate these issues, it is essential to adopt practices like data augmentation, transfer learning, and adversarial debiasing techniques. Moreover, developing more diverse datasets and promoting greater transparency around model development processes can help reduce bias and improve overall performance. \n",
    "\n",
    "1. **Diverse and Representative Training Data:**\n",
    "   - Ensure training datasets represent various demographics, cultures, and perspectives.\n",
    "   - Identify and rectify underrepresentation.\n",
    "\n",
    "2. **Bias Detection and Mitigation Algorithms:**\n",
    "   - Implement algorithms to detect and mitigate biases in model outputs.\n",
    "   - Regularly audit and monitor for biases.\n",
    "\n",
    "3. **Fine-Tuning on Domain-Specific Data:**\n",
    "   - Adapt pretrained models to context-specific nuances through fine-tuning.\n",
    "   - Improve performance in specific areas and reduce the risk of biases.\n",
    "\n",
    "4. **Explainability and Transparency:**\n",
    "   - Enhance transparency by providing explanations for model decisions.\n",
    "   - Allow users to assess and address model limitations effectively.\n",
    "\n",
    "5. **Incorporating Ethical Guidelines:**\n",
    "   - Implement and adhere to ethical guidelines in model development and deployment.\n",
    "   - Consider societal impact and ethical standards.\n",
    "\n",
    "6. **User Feedback and Iterative Improvement:**\n",
    "   - Encourage user feedback and incorporate it into iterative model updates.\n",
    "   - Refine models based on real-world usage experiences.\n",
    "\n",
    "7. **Cross-disciplinary Collaboration:**\n",
    "   - Engage experts from diverse fields, including ethics, sociology, and cultural studies.\n",
    "   - Bring valuable insights to identify and rectify biases.\n",
    "\n",
    "8. **Robust Evaluation Metrics:**\n",
    "   - Develop and utilize comprehensive evaluation metrics beyond traditional accuracy.\n",
    "   - Gain a nuanced understanding of model performance in different scenarios.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
