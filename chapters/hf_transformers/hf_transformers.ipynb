{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Huggingface Transformers\n",
        "Beginner-focused Jupyter Notebook introducing Hugging Face Transformer pipelines: Learn essentials of text embeddings, classification, question answering, named entity recognition and text generation using simple steps and pre-trained models. No prerequisites!\n",
        "\n",
        "## What are Transformers?\n",
        "\n",
        "Natural Language Processing is a field of linguistics and machine learning focused on understanding everything related to human language. The aim of NLP tasks is not only to understand single words individually, but to be able to understand the context of those words.\n",
        "\n",
        "The following is a list of common NLP tasks, with some examples of each:\n",
        "\n",
        "- Sentiment Analysis\n",
        "- Question Answering\n",
        "- Named Entity Recognition\n",
        "- Text Summarization\n",
        "- Text Generation\n",
        "- Text Classification: Zero-shot, Multi-label\n",
        "- Mask Filling\n",
        "- Text Translation\n",
        "\n",
        "\n",
        "**Transformer models** can be used to solve all kinds of NLP tasks, like the ones mentioned above. The ðŸ¤— Transformers library provides the functionality to create and use those shared models. The Model Hub contains thousands of pretrained models that anyone can download and use. You can also upload your own models to the Hub!\n",
        "\n",
        "\n",
        "In this notebook, we will use the `transformers` library to use pre-trained models for various NLP tasks. We will use the `pipeline` class to use pre-trained models for various NLP tasks. The `pipeline` class provides a simple API dedicated to several NLP tasks. It provides a simple, straight-forward, and efficient way to use pre-trained models.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/miztiik/llm-bootcamp/blob/main/chapters/hf_transformers/hf_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWymbqNSTosA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Comment the above line to see the installation logs\n",
        "\n",
        "# Install the latest version of the transformers library\n",
        "!pip install -qU transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline\n",
        "The most basic object in the ðŸ¤— Transformers library is the `pipeline()` function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer.\n",
        "\n",
        "![Pipeline](images/transformer-pipeline.png)\n",
        "\n",
        "Refer to the [Huggingface Pipelines](https://huggingface.co/docs/transformers/en/main_classes/pipelines) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sentiment analysis pipeline\n",
        "\n",
        "<img src=\"images/sentiment_analysis.png\" width=\"50%\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentiment_task = pipeline(\"sentiment-analysis\")\n",
        "sentences = [\n",
        "    \"Dr Stone is great anime.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "sentiment_task(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pipelines groups together three steps: preprocessing, passing the inputs through the model, and postprocessing:\n",
        "<img src=\"images/sentiment_analysis_01.png\" width=\"50%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is best practice to setup a cache directory for the transformers library. This will allow us to reuse the downloaded models and avoid downloading them again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Setting the Hugging Face Model Cache Directory\n",
        "MODEL_CACHE_DIR = \"./../../../models_cache\"\n",
        "\n",
        "# https://stackoverflow.com/questions/63312859/how-to-change-huggingface-transformers-default-cache-directory\n",
        "os.environ[\"HF_HOME\"] = MODEL_CACHE_DIR\n",
        "os.environ[\"TRANSFORMERS_CACHE\"] = MODEL_CACHE_DIR\n",
        "\n",
        "# os.environ[\"HF_DATASETS_CACHE\"] = f\"{MODEL_CACHE_DIR}/datasets\"\n",
        "# os.environ[\"TORCH_HOME\"] = f\"{MODEL_CACHE_DIR}/torch\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Sentiment Analysis with Custom Model**\n",
        "\n",
        "Lets choose a model from the [Model Hub: Text Classification](https://huggingface.co/models?pipeline_tag=text-classification&sort=likes) or Search for a `sentiment` analysis model https://huggingface.co/models?search=sentiment\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest\n",
        "\n",
        "sentiment_model = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "sentiment_task = pipeline(\n",
        "    \"sentiment-analysis\", model=sentiment_model, tokenizer=sentiment_model\n",
        ")\n",
        "sentences = [\n",
        "    \"Dark Knight Rises was a great movie!.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "sentiment_task(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question Answering \n",
        "\n",
        "QA systems differ in the way answers are created.\n",
        "\n",
        "- **Extractive QnA**: The model extracts the answer from a context and provides it directly to the user. It is usually solved with BERT-like models.\n",
        "\n",
        "  <img src=\"images/ex_q_a_01.png\" width=50%>\n",
        "- **Generative QnA**: The model generates free text directly based on the context. It leverages Text Generation models and provides more flexible answers.\n",
        "\n",
        "  <img src=\"images/gen_q_a_01.png\" width=50%>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "qa_model = pipeline(\"question-answering\")\n",
        "\n",
        "question = \"What is zero?\"\n",
        "context = \"zero is a number representing an empty quantity. Adding zero to any number leaves that number unchanged.  Multiplying any number by zero has the result zero, and consequently, division by zero has no meaning in arithmetic.\"\n",
        "\n",
        "qa_response = qa_model(question=question, context=context)\n",
        "print(qa_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model returns a dictionary containing the keys:\n",
        "\n",
        "`answer`: The text extracted from the context, which should contain the answer.\n",
        "\n",
        "`start`: The index of the character in the context that corresponds to the start of the extracted answer.\n",
        "\n",
        "`end`: The index of the character in the context that corresponds to the end of the extracted answer.\n",
        "\n",
        "`score`: The confidence of the model in extracting the answer from the context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Named Entity Recognition\n",
        "\n",
        "Named Entity Recognition (NER) is the NLP task of identifying key information (entities) in text. An entity is a set of contiguous words that appear in the document and refers to the same thing. Some examples of entities are \"Kumar\", \"India\". Usually, entities are classified into categories like `Name`, `Country`.\n",
        "\n",
        "  <img src=\"images/ner.png\" width=50%>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ner = pipeline(\"ner\")\n",
        "ner(\"What was Gandhi doing in India on 15 August 1947?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ner = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
        "ner(\"What was Gandhi and Nehru doing in India on 15 August 1947?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_O4rhgNMhL5"
      },
      "source": [
        "### Text Summarization\n",
        "\n",
        "Text summarization is a task whose goal is generating a concise and precise summary of long texts, without losing the overall meaning. There are two main approaches to automatic text summarization: Abstractive summarization and Extractive summarization.\n",
        "\n",
        "- **Extraction-based summarization**: A subset of words or sentences that represent the most important points is pulled from the long text and combined to make a summary. The results may not be grammatically accurate.\n",
        "\n",
        "- **Abstraction-based summarization**: Advanced deep learning techniques (mainly in seq-to-seq models) are applied to paraphrase and shorten the original document, just like humans do. Since abstractive machine learning algorithms can generate new phrases and sentences that represent the most important information from the source text, they can assist in overcoming the grammatical inaccuracies of the extraction-based techniques.\n",
        "\n",
        "<img src=\"images/text_summarization.png\" width=50%>\n",
        "\n",
        "Try Text Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaGO6aX7Z8ht",
        "outputId": "8c329d00-a34c-4609-8928-e1997a17d4e1"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "text = \"\"\"\n",
        "The Uttaramerur inscriptions describe the distinctive Kudavolai system employed in the local governance of the Chola kingdom. In this administrative method, one representative per ward was selected via an electoral process. Contestant names were recorded on palm leaf tickets, which were subsequently placed inside a pot and mixed thoroughly. A young child drew out the tickets randomly, announcing the names of those chosen as representatives. Through this procedure, thirty individuals were elected to serve as ward administrators. The intriguing election technique came to be known as the Kudavolai system.\n",
        "\"\"\"\n",
        "summarizer(text, max_length=100, min_length=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Generation\n",
        "\n",
        "<img src=\"images/text_gen.png\" width=50%>\n",
        "\n",
        "Try Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLwpCu3VL0r2",
        "outputId": "4072fbe8-8aa1-40e6-91d9-e3c088745030"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\")\n",
        "generator(\"In this course, we will teach you how to\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can control how many different sequences are generated with the argument `num_return_sequences` and the total length of the output text with the argument `max_length`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generator(\"Science fiction movies are based on \",\n",
        "          max_length=15, num_return_sequences=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Classification: Zero-shot classification\n",
        "\n",
        "**What is Zero-Shot Learning?**\n",
        "Zero-Shot Learning is the ability to detect classes that the model has never seen during training. It resembles our ability as humans to generalize and identify new things without explicit supervision.\n",
        "\n",
        "For example, letâ€™s say we want to do `sentiment classification` and `news category` classification. Normally, we will train/fine-tune a new model for each dataset. In contrast, with zero-shot learning, you can perform tasks such as sentiment and news classification directly without any task-specific training.\n",
        "\n",
        "<img src=\"images/zero-shot-vs-transfer.png\" width=50%>\n",
        "\n",
        "Try Text Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "txt_to_classify = \"You are learning about transformer library in this notebook. It is part of a course on LLM Bootcamp.\"\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "classifier(\n",
        "    txt_to_classify,\n",
        "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Classification: Multi-label classification\n",
        "If more than one candidate label can be correct, pass multi_label=True to calculate each class independently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "txt_to_classify = \"I love travelling and enjoying the cuisines of the world.\"\n",
        "candidate_labels = [\"travel\", \"cooking\", \"dancing\", \"exploration\"]\n",
        "classifier(txt_to_classify, candidate_labels, multi_label=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mask filling\n",
        "The model attempts to fills in the special `<mask>` word, which is often referred to as a _mask token_. Other mask-filling models might have different mask tokens, so itâ€™s always good to verify the proper mask word when exploring other models. One way to check it is by looking at the mask word used in the widget.The `top_k` argument controls how many possibilities you want to be displayed. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\")\n",
        "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Translation\n",
        "<img src=\"images/text_translation_03.png\" width=50%>\n",
        "<img src=\"images/text_translation_02.png\" width=50%>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "from transformers import pipeline\n",
        "\n",
        "# French to English\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
        "translator(\n",
        "    \"Apprendre est amusant, j'aime apprendre l'intelligence artificielle.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Addtional Reading\n",
        "\n",
        "1.  [Huggingface Transformers](https://huggingface.co/transformers/)\n",
        "1.  [Huggingface Transformers Github](https://github.com/huggingface/transformers)\n",
        "2.  [Huggingface Model Hub](https://huggingface.co/models)\n",
        "3.  [LLM Bootcamp](https://github.com/miztiik/llm-bootcamp)\n",
        "4.  [Deep dive into Text Generation](https://huggingface.co/blog/how-to-generate)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMSmvGQDAV4t8zAS+BmSIfl",
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
