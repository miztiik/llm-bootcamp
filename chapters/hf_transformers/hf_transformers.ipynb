{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Huggingface Transformers\n",
        "Beginner-focused Jupyter Notebook introducing Hugging Face Transformer pipelines: Learn essentials of text embeddings, classification, question answering, named entity recognition and text generation using simple steps and pre-trained models. No prerequisites!\n",
        "\n",
        "## What are Transformers?\n",
        "\n",
        "Natural Language Processing is a field of linguistics and machine learning focused on understanding everything related to human language. The aim of NLP tasks is not only to understand single words individually, but to be able to understand the context of those words.\n",
        "\n",
        "The following is a list of common NLP tasks, with some examples of each:\n",
        "\n",
        "- Sentiment Analysis\n",
        "- Question Answering\n",
        "- Named Entity Recognition\n",
        "- Text Summarization\n",
        "- Text Generation\n",
        "- Text Classification: Zero-shot, Multi-label\n",
        "- Mask Filling\n",
        "- Text Translation\n",
        "\n",
        "\n",
        "**Transformer models** can be used to solve all kinds of NLP tasks, like the ones mentioned above. The ðŸ¤— Transformers library provides the functionality to create and use those shared models. The Model Hub contains thousands of pretrained models that anyone can download and use. You can also upload your own models to the Hub!\n",
        "\n",
        "**Resources:**\n",
        "1.  [Huggingface Transformers](https://huggingface.co/transformers/)\n",
        "2.  [Huggingface Model Hub](https://huggingface.co/models)\n",
        "3.  [LLM Bootcamp](https://github.com/miztiik/llm-bootcamp)\n",
        "\n",
        "In this notebook, we will use the `transformers` library to use pre-trained models for various NLP tasks. We will use the `pipeline` class to use pre-trained models for various NLP tasks. The `pipeline` class provides a simple API dedicated to several NLP tasks. It provides a simple, straight-forward, and efficient way to use pre-trained models.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/miztiik/llm-bootcamp/chapters/hf_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OWymbqNSTosA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Comment the above line to see the installation logs\n",
        "\n",
        "# Install the latest version of the transformers library\n",
        "!pip install -qU transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline\n",
        "The most basic object in the ðŸ¤— Transformers library is the `pipeline()` function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer.\n",
        "\n",
        "![Pipeline](images/transformer-pipeline.png)\n",
        "\n",
        "Refer to the [Huggingface Pipelines](https://huggingface.co/docs/transformers/en/main_classes/pipelines) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sentiment analysis pipeline\n",
        "\n",
        "<img src=\"images/sentiment_analysis.png\" width=\"50%\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.9907229542732239}]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentiment_task = pipeline(\"sentiment-analysis\")\n",
        "sentences = [\n",
        "    \"Dr Stone is great anime.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "sentiment_task(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pipelines groups together three steps: preprocessing, passing the inputs through the model, and postprocessing:\n",
        "<img src=\"images/sentiment_analysis_01.png\" width=\"50%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is best practice to setup a cache directory for the transformers library. This will allow us to reuse the downloaded models and avoid downloading them again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Setting the Hugging Face Model Cache Directory\n",
        "MODEL_CACHE_DIR = \"./../../../models_cache\"\n",
        "\n",
        "# https://stackoverflow.com/questions/63312859/how-to-change-huggingface-transformers-default-cache-directory\n",
        "os.environ[\"HF_HOME\"] = MODEL_CACHE_DIR\n",
        "os.environ[\"TRANSFORMERS_CACHE\"] = MODEL_CACHE_DIR\n",
        "\n",
        "# os.environ[\"HF_DATASETS_CACHE\"] = f\"{MODEL_CACHE_DIR}/datasets\"\n",
        "# os.environ[\"TORCH_HOME\"] = f\"{MODEL_CACHE_DIR}/torch\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Sentiment Analysis with Custom Model**\n",
        "\n",
        "Lets choose a model from the [Model Hub: Text Classification](https://huggingface.co/models?pipeline_tag=text-classification&sort=likes) or Search for a `sentiment` analysis model https://huggingface.co/models?search=sentiment\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'positive', 'score': 0.97898930311203},\n",
              " {'label': 'negative', 'score': 0.9291250109672546}]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest\n",
        "\n",
        "sentiment_model = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
        "sentiment_task = pipeline(\n",
        "    \"sentiment-analysis\", model=sentiment_model, tokenizer=sentiment_model\n",
        ")\n",
        "sentences = [\n",
        "    \"Dark Knight Rises was a great movie!.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "sentiment_task(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question Answering \n",
        "\n",
        "QA systems differ in the way answers are created.\n",
        "\n",
        "- **Extractive QnA**: The model extracts the answer from a context and provides it directly to the user. It is usually solved with BERT-like models.\n",
        "  <img src=\"images/ex_q_a_01.png\" width=50%>\n",
        "- **Generative QnA**: The model generates free text directly based on the context. It leverages Text Generation models and provides more flexible answers.\n",
        "  <img src=\"images/gen_q_a_01.png\" width=50%>\n",
        "\n",
        "Try Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'score': 0.8385463953018188, 'start': 8, 'end': 47, 'answer': 'a number representing an empty quantity'}\n"
          ]
        }
      ],
      "source": [
        "qa_model = pipeline(\"question-answering\")\n",
        "\n",
        "question = \"What is zero?\"\n",
        "context = \"zero is a number representing an empty quantity. Adding zero to any number leaves that number unchanged.  Multiplying any number by zero has the result zero, and consequently, division by zero has no meaning in arithmetic.\"\n",
        "\n",
        "qa_response = qa_model(question=question, context=context)\n",
        "print(qa_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model returns a dictionary containing the keys:\n",
        "\n",
        "`answer`: The text extracted from the context, which should contain the answer.\n",
        "\n",
        "`start`: The index of the character in the context that corresponds to the start of the extracted answer.\n",
        "\n",
        "`end`: The index of the character in the context that corresponds to the end of the extracted answer.\n",
        "\n",
        "`score`: The confidence of the model in extracting the answer from the context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Named Entity Recognition\n",
        "\n",
        "Named Entity Recognition (NER) is the NLP task of identifying key information (entities) in text. An entity is a set of contiguous words that appear in the document and refers to the same thing. Some examples of entities are \"Kumar\", \"India\". Usually, entities are classified into categories like `Name`, `Country`.\n",
        "\n",
        "  <img src=\"images/ner.png\" width=50%>\n",
        "\n",
        "Try NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'entity': 'I-PER',\n",
              "  'score': 0.9981331,\n",
              "  'index': 3,\n",
              "  'word': 'Gandhi',\n",
              "  'start': 9,\n",
              "  'end': 15},\n",
              " {'entity': 'I-LOC',\n",
              "  'score': 0.99966097,\n",
              "  'index': 6,\n",
              "  'word': 'India',\n",
              "  'start': 25,\n",
              "  'end': 30}]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ner = pipeline(\"ner\")\n",
        "ner(\"What was Gandhi doing in India on 15 August 1947?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'entity_group': 'PER',\n",
              "  'score': 0.9975593,\n",
              "  'word': 'Gandhi',\n",
              "  'start': 9,\n",
              "  'end': 15},\n",
              " {'entity_group': 'PER',\n",
              "  'score': 0.99828976,\n",
              "  'word': 'Nehru',\n",
              "  'start': 20,\n",
              "  'end': 25},\n",
              " {'entity_group': 'LOC',\n",
              "  'score': 0.99964774,\n",
              "  'word': 'India',\n",
              "  'start': 35,\n",
              "  'end': 40}]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ner = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
        "ner(\"What was Gandhi and Nehru doing in India on 15 August 1947?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_O4rhgNMhL5"
      },
      "source": [
        "### Text Summarization\n",
        "\n",
        "Text summarization is a task whose goal is generating a concise and precise summary of long texts, without losing the overall meaning. There are two main approaches to automatic text summarization: Abstractive summarization and Extractive summarization.\n",
        "\n",
        "- **Extraction-based summarization**: A subset of words or sentences that represent the most important points is pulled from the long text and combined to make a summary. The results may not be grammatically accurate.\n",
        "\n",
        "- **Abstraction-based summarization**: Advanced deep learning techniques (mainly in seq-to-seq models) are applied to paraphrase and shorten the original document, just like humans do. Since abstractive machine learning algorithms can generate new phrases and sentences that represent the most important information from the source text, they can assist in overcoming the grammatical inaccuracies of the extraction-based techniques.\n",
        "\n",
        "<img src=\"images/text_summarization.png\" width=50%>\n",
        "\n",
        "Try Text Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaGO6aX7Z8ht",
        "outputId": "8c329d00-a34c-4609-8928-e1997a17d4e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'summary_text': ' Uttaramerur inscriptions describe the distinctive Kudavolai system employed in the local governance of the Chola kingdom . In this administrative method, one representative per ward was selected via an electoral process . Contestant names were recorded on palm leaf tickets, which were placed inside a pot and mixed thoroughly .'}]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "text = \"\"\"\n",
        "The Uttaramerur inscriptions describe the distinctive Kudavolai system employed in the local governance of the Chola kingdom. In this administrative method, one representative per ward was selected via an electoral process. Contestant names were recorded on palm leaf tickets, which were subsequently placed inside a pot and mixed thoroughly. A young child drew out the tickets randomly, announcing the names of those chosen as representatives. Through this procedure, thirty individuals were elected to serve as ward administrators. The intriguing election technique came to be known as the Kudavolai system.\n",
        "\"\"\"\n",
        "summarizer(text, max_length=100, min_length=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Generation\n",
        "\n",
        "<img src=\"images/text_gen.png\" width=50%>\n",
        "\n",
        "Try Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLwpCu3VL0r2",
        "outputId": "4072fbe8-8aa1-40e6-91d9-e3c088745030"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'In this course, we will teach you how to solve many real-world issues and learn how using real-world problems can dramatically reduce your risk of illness.\\n\\nThis course will provide you with the basics, along with some of the most effective'}]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\")\n",
        "generator(\"In this course, we will teach you how to\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can control how many different sequences are generated with the argument `num_return_sequences` and the total length of the output text with the argument `max_length`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'Science fiction movies are based on \\xa0a series of novels and other novels'},\n",
              " {'generated_text': 'Science fiction movies are based on vernacular translations or, in contemporary comics'}]"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generator(\"Science fiction movies are based on \",\n",
        "          max_length=15, num_return_sequences=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Classification: Zero-shot classification\n",
        "\n",
        "**What is Zero-Shot Learning?**\n",
        "Zero-Shot Learning is the ability to detect classes that the model has never seen during training. It resembles our ability as humans to generalize and identify new things without explicit supervision.\n",
        "\n",
        "For example, letâ€™s say we want to do `sentiment classification` and `news category` classification. Normally, we will train/fine-tune a new model for each dataset. In contrast, with zero-shot learning, you can perform tasks such as sentiment and news classification directly without any task-specific training.\n",
        "\n",
        "<img src=\"images/zero-shot-vs-transfer.png\" width=50%>\n",
        "\n",
        "Try Text Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to facebook/bart-large-mnli and revision c626438 (https://huggingface.co/facebook/bart-large-mnli).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'sequence': 'You are learning about transformer library in this notebook. It is part of a course on LLM Bootcamp.',\n",
              " 'labels': ['education', 'business', 'politics'],\n",
              " 'scores': [0.969709038734436, 0.02318527176976204, 0.007105606608092785]}"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "txt_to_classify = \"You are learning about transformer library in this notebook. It is part of a course on LLM Bootcamp.\"\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "classifier(\n",
        "    txt_to_classify,\n",
        "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Classification: Multi-label classification\n",
        "If more than one candidate label can be correct, pass multi_label=True to calculate each class independently:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'classifier' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m txt_to_classify \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI love travelling and enjoying the cuisines of the world.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m candidate_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtravel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcooking\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdancing\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexploration\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m \u001b[43mclassifier\u001b[49m(txt_to_classify, candidate_labels, multi_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'classifier' is not defined"
          ]
        }
      ],
      "source": [
        "txt_to_classify = \"I love travelling and enjoying the cuisines of the world.\"\n",
        "candidate_labels = [\"travel\", \"cooking\", \"dancing\", \"exploration\"]\n",
        "classifier(txt_to_classify, candidate_labels, multi_label=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mask filling\n",
        "\n",
        "The `top_k` argument controls how many possibilities you want to be displayed. Note that here the model fills in the special `<mask>` word, which is often referred to as a _mask token_. Other mask-filling models might have different mask tokens, so itâ€™s always good to verify the proper mask word when exploring other models. One way to check it is by looking at the mask word used in the widget.\n",
        "\n",
        "<img src=\"images/zero-shot-vs-transfer.png\" width=50%>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilroberta-base and revision ec58a5b (https://huggingface.co/distilroberta-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f8e16ec12594dd69ff86d6f222fecf2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d435ae706ada4ee281321f35947bc4d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea9d649a095842aabaef0ee0fe34f7d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd00e4c197524c378f6331e080f65868",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6bc55185e0a44edaeee97f6387bb26f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "[{'score': 0.1961977779865265,\n",
              "  'token': 30412,\n",
              "  'token_str': ' mathematical',\n",
              "  'sequence': 'This course will teach you all about mathematical models.'},\n",
              " {'score': 0.04052717983722687,\n",
              "  'token': 38163,\n",
              "  'token_str': ' computational',\n",
              "  'sequence': 'This course will teach you all about computational models.'}]"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\")\n",
        "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Translation\n",
        "<img src=\"images/text_translation_01.jpg\" width=50%>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 328 ms\n",
            "Wall time: 1.57 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'translation_text': 'Learning is fun, I like learning artificial intelligence.'}]"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "from transformers import pipeline\n",
        "\n",
        "# French to English\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
        "translator(\n",
        "    \"Apprendre est amusant, j'aime apprendre l'intelligence artificielle.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMSmvGQDAV4t8zAS+BmSIfl",
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
