{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Embeddings & Semantic Search\n",
    "\n",
    "This notebook focuses on Embeddings and Vectorstores. We'll look into different embedding functions and compare them. We'll look into vectorstores, build an index, look into similarity scores and practice an async db querying!\n",
    "\n",
    "<img src=\"images/sentiment_analysis.png\" width=\"50%\"/>\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/miztiik/llm-bootcamp/blob/main/chapters/hf_transformers/hf_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Comment the above line to see the installation logs\n",
    "\n",
    "# Install the dependencies\n",
    "!pip install -qU python-dotenv\n",
    "!pip install -qU langchain\n",
    "!pip install -qU langchain-openai\n",
    "!pip install -qU numpy\n",
    "!pip install -qU sns\n",
    "!pip install -qU matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not a good practice, but we will ignore warnings in this notebook, as tensor has deprecated some methods and will be removed in future versions.\n",
    "# https://github.com/pytorch/pytorch/issues/97207#issuecomment-1494781560\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", category=UserWarning, message=\"TypedStorage is deprecated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update your `API_KEY` in the `.env` file. You can get the API keys from the following links. \n",
    "\n",
    "If you are running in google colab, add the key to the secrets manager under the \"ðŸ”‘\" in the left panel. Give it the name `SVC_NAME_API_KEY`.\n",
    "\n",
    "_Note: Some of the services may require you to have an account and some may charge you for usage_\n",
    "- [OpenAI API Key](https://platform.openai.com/account/api-keys)\n",
    "- [Hugging Face API Key](https://huggingface.co/settings/tokens)\n",
    "- [Serper API Key](https://serper.dev/api-key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# To specify a particular model refer to the OpenAI documentation - https://platform.openai.com/docs/models\n",
    "# Completions Model: https://platform.openai.com/docs/models/completions\n",
    "# Chat Model: https://platform.openai.com/docs/models/completions\n",
    "\n",
    "llm = OpenAI()\n",
    "llm_chat = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\", temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "In this section, we generate embeddings for a list of sentences using different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Best travel neck pillow for long flights\",\n",
    "    \"Lightweight backpack for hiking and travel\",\n",
    "    \"Waterproof duffel bag for outdoor adventures\",\n",
    "    \"Stainless steel cookware set for induction cooktops\",\n",
    "    \"High-quality chef's knife set\",\n",
    "    \"High-performance stand mixer for baking\",\n",
    "    \"New releases in fiction literature\",\n",
    "    \"Inspirational biographies and memoirs\",\n",
    "    \"Top self-help books for personal growth\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of embeddings\n",
    "text_embedding_list = [hf_embeddings.embed_query(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how our embedding looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding_list[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_embedding_list[0]), len(text_embedding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity(\n",
    "    [text_embedding_list[0]], [text_embedding_list[1]]\n",
    "), cosine_similarity([text_embedding_list[0]], [text_embedding_list[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see, the cosine_similarity metric for `sentences[0]` and `sentences[1] (0.445)` is much higher than between `sentences[0]` and `sentences[9] (0.08)`, symbolyzing much higher similarity.\n",
    "\n",
    "Indeed, sentence _Best travel neck pillow for long flights_ has more in common with _Lightweight backpack for hiking and travel_ than _Top self-help books for personal growth_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of sentences\n",
    "import pandas as pd\n",
    "num_sentences = len(sentences)\n",
    "\n",
    "# Initialize a similarity matrix with zeros\n",
    "similarity_matrix = np.zeros((num_sentences, num_sentences))\n",
    "\n",
    "# Calculate the cosine similarity between sentence embeddings\n",
    "# Iterate through the upper triangular part of the matrix\n",
    "for i in range(num_sentences):\n",
    "    for j in range(i + 1, num_sentences):\n",
    "        # Retrieve embeddings for the current pair of sentences\n",
    "        embedding_i, embedding_j = text_embedding_list[i], text_embedding_list[j]\n",
    "\n",
    "        # Calculate and assign the cosine similarity between the embeddings\n",
    "        similarity_matrix[i, j] = cosine_similarity(\n",
    "            [embedding_i], [embedding_j])\n",
    "\n",
    "# Copy the values from the upper triangular part to the lower triangular part\n",
    "similarity_matrix += similarity_matrix.T\n",
    "\n",
    "# Fill the diagonal of the similarity matrix with ones, indicating self-similarity\n",
    "np.fill_diagonal(similarity_matrix, 1)\n",
    "# Create a heatmap using Seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(similarity_matrix, xticklabels=False, yticklabels=False)\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Embedding Heatmap\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "EMBEDDING_SIZE = 75\n",
    "CHUNK_SIZE = EMBEDDING_SIZE // 3\n",
    "sorted_var_indexes = np.argsort(np.var(text_embedding_list, axis=0))\n",
    "filtered_embedding_values = np.array(np.array(text_embedding_list).T)[\n",
    "    sorted_var_indexes[-EMBEDDING_SIZE:]\n",
    "]\n",
    "_df = pd.DataFrame(filtered_embedding_values)\n",
    "_df.head(3)\n",
    "sort_indexes = list(_df[[0, 1, 2]].mean(\n",
    "    axis=1).sort_values().index)[:CHUNK_SIZE]\n",
    "sort_indexes += [\n",
    "    i for i in _df[[3, 4, 5]].mean(axis=1).sort_values().index if i not in sort_indexes\n",
    "][:CHUNK_SIZE]\n",
    "sort_indexes += [\n",
    "    i for i in _df[[6, 7, 8]].mean(axis=1).sort_values().index if i not in sort_indexes\n",
    "]\n",
    "sorted_text_embedding_list = filtered_embedding_values[sort_indexes]\n",
    "# Create a heatmap using Seaborn\n",
    "plt.figure(figsize=(8, 3))\n",
    "sns.heatmap(sorted_text_embedding_list.T, xticklabels=False, yticklabels=False)\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Embeddings with 75 dimensions\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Reading\n",
    "\n",
    "- [LLM Bootcamp](https://github.com/miztiik/llm-bootcamp)\n",
    "- [Evaluating the Ideal Chunk Size for a RAG System using LlamaIndex](https://blog.llamaindex.ai/evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5) Colab: <a href=\"https://colab.research.google.com/drive/1LPvJyEON6btMpubYdwySfNs0FuNR9nza?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
